{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling parl-1.1.2:\n",
      "  Successfully uninstalled parl-1.1.2\n",
      "Uninstalling pandas-0.23.4:\n",
      "  Successfully uninstalled pandas-0.23.4\n",
      "Uninstalling scikit-learn-0.20.0:\n",
      "  Successfully uninstalled scikit-learn-0.20.0\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: gym in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.12.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (2.22.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.16.4)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.0->gym) (1.25.6)\n",
      "Requirement already satisfied: future in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyglet>=1.2.0->gym) (0.18.0)\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting atari-py\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='mirrors.ustc.edu.cn', port=443): Read timed out. (read timeout=15)\")': /pypi/web/simple/atari-py/\u001b[0m\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/8f/ba/1d22e9d2f332f07aaa57041f5dd569c2cb40a92bd6374a0b743ec3dfae97/atari_py-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8MB 458kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from atari-py) (1.16.4)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from atari-py) (1.12.0)\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-0.2.6\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting paddlepaddle==1.6.3\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/96/28/e72bebb3c9b3d98eb9b15d9f6d85150f3cbd63e695e59882ff9f04846686/paddlepaddle-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (90.9MB)\n",
      "\u001b[K     |████████████████████████████████| 90.9MB 67kB/s eta 0:00:0112    |████▊                           | 13.6MB 23kB/s eta 0:55:31     |█████████████████████████▋      | 72.9MB 108kB/s eta 0:02:46\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.22.0)\n",
      "Requirement already satisfied: objgraph in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (6.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.4.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (5.1.2)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.2.3)\n",
      "Requirement already satisfied: prettytable in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.7.2)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.12.0)\n",
      "Requirement already satisfied: rarfile in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.1)\n",
      "Requirement already satisfied: scipy; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.3.0)\n",
      "Requirement already satisfied: funcsigs in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.0.2)\n",
      "Requirement already satisfied: graphviz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.13)\n",
      "Requirement already satisfied: numpy>=1.12; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.16.4)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.10.0)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.1.1.26)\n",
      "Requirement already satisfied: nltk; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (0.10.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2019.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from protobuf>=3.1.0->paddlepaddle==1.6.3) (41.4.0)\n",
      "Installing collected packages: paddlepaddle\n",
      "  Found existing installation: paddlepaddle 1.7.1\n",
      "    Uninstalling paddlepaddle-1.7.1:\n",
      "      Successfully uninstalled paddlepaddle-1.7.1\n",
      "Successfully installed paddlepaddle-1.6.3\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting parl==1.3.1\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/62/79/590af38a920792c71afb73fad7583967928b4d0ba9fca76250d935c7fda8/parl-1.3.1-py2.py3-none-any.whl (521kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 9.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: flask>=1.0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.1.1)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (7.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: pyzmq==18.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (18.0.1)\n",
      "Requirement already satisfied: cloudpickle==1.2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.2.1)\n",
      "Collecting flask-cors (from parl==1.3.1)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting psutil>=5.6.2 (from parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c4/b8/3512f0e93e0db23a71d82485ba256071ebef99b227351f0f5540f744af41/psutil-5.7.0.tar.gz (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 47.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboardX==1.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.8)\n",
      "Collecting visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\" (from parl==1.3.1)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/visualdl/\u001b[0m\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/1e/1e/d92ce71705b0de5e5fde210fc7369ee9a1aa6a53065a83c968b655885b9a/visualdl-2.0.0b6-py3-none-any.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 45.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: pyarrow==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (0.13.0)\n",
      "Requirement already satisfied: tb-nightly==1.15.0a20190801 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.15.0a20190801)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (2.10.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scipy>=1.0.0->parl==1.3.1) (1.16.4)\n",
      "Requirement already satisfied: Six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask-cors->parl==1.3.1) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboardX==1.8->parl==1.3.1) (3.10.0)\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (3.7.9)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (4.1.1.26)\n",
      "Collecting hdfs (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 28.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.21.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.22.0)\n",
      "Collecting Pillow>=7.0.0 (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/ab/f8/d3627cc230270a6a4eedee32974fbc8cb26c5fdb8710dd5ea70133640022/Pillow-7.1.2-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 46.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Flask-Babel>=1.0.0 (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/76/a4/0115c7c520125853037fc1d6b3da132a526949640e27a699a13e05ec7593/Flask_Babel-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (41.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (3.1.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.33.6)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (1.26.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.0.4->parl==1.3.1) (1.1.1)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.6.1)\n",
      "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.5.0)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.3)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.1.1)\n",
      "Collecting docopt (from hdfs->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.0.1)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.3.4)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (16.7.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.23)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.4.10)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (5.1.2)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.8)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2019.3)\n",
      "Collecting Babel>=2.3 (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1)\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/15/a1/522dccd23e5d2e47aed4b6a16795b8213e3272c7506e625f2425ad025a19/Babel-2.8.0-py2.py3-none-any.whl (8.6MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6MB 234kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (7.2.0)\n",
      "Building wheels for collected packages: psutil, hdfs, docopt\n",
      "  Building wheel for psutil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psutil: filename=psutil-5.7.0-cp37-cp37m-linux_x86_64.whl size=261258 sha256=f65156733f67ba4ae6819a92637e6147625b983159ec40eaed178645f53e80d7\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/fd/8b/7e/939524c38be886652fe8b1688384da4bafe0a8224d504e90eb\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.5.8-cp37-none-any.whl size=33214 sha256=89fbb8e6f87973a8b1002e85e3d6e6b01ac9ebe68a1934d10499b289f3c6fe6a\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/35/91/05/ed325f80520cc72b4eaa7327f96358c62d84afd098625ed2bd\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=b49841f8052756fa7fe0f88d2ccf8528cf4a333da7da266a64bc83bb45146b1a\n",
      "  Stored in directory: /home/aistudio/.cache/pip/wheels/78/da/5a/be54433e626178926da00dbc53e06294ba87ec2c37dded83b4\n",
      "Successfully built psutil hdfs docopt\n",
      "\u001b[31mERROR: paddlehub 1.6.0 requires pandas; python_version >= \"3\", which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: visualdl 2.0.0b6 has requirement six>=1.14.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: flask-cors, psutil, docopt, hdfs, Pillow, Babel, Flask-Babel, visualdl, parl\n",
      "  Found existing installation: Pillow 6.2.0\n",
      "    Uninstalling Pillow-6.2.0:\n",
      "      Successfully uninstalled Pillow-6.2.0\n",
      "  Found existing installation: visualdl 1.3.0\n",
      "    Uninstalling visualdl-1.3.0:\n",
      "      Successfully uninstalled visualdl-1.3.0\n",
      "Successfully installed Babel-2.8.0 Flask-Babel-1.0.0 Pillow-7.1.2 docopt-0.6.2 flask-cors-3.0.8 hdfs-2.5.8 parl-1.3.1 psutil-5.7.0 visualdl-2.0.0b6\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y parl  # 说明：AIStudio预装的parl版本太老，容易跟其他库产生兼容性冲突，建议先卸载\n",
    "!pip uninstall -y pandas scikit-learn # 提示：在AIStudio中卸载这两个库再import parl可避免warning提示，不卸载也不影响parl的使用\n",
    "\n",
    "!pip install gym\n",
    "!pip install atari-py # 玩Gym的Atari游戏必装依赖，本次作业使用了Atari的Pong(乒乓球)环境\n",
    "!pip install paddlepaddle==1.6.3\n",
    "!pip install parl==1.3.1\n",
    "\n",
    "# 说明：安装日志中出现两条红色的关于 paddlehub 和 visualdl 的 ERROR 与parl无关，可以忽略，不影响使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlepaddle         1.6.3          \n",
      "parl                 1.3.1          \n"
     ]
    }
   ],
   "source": [
    "# 检查依赖包版本是否正确\n",
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "import parl\n",
    "from parl import layers\n",
    "from parl.utils import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 1. 请设定 learning rate，尝试增减查看效果\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "\n",
    "LEARNING_RATE = 0.004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 搭建Model、Algorithm、Agent架构\n",
    "* `Agent`把产生的数据传给`algorithm`，`algorithm`根据`model`的模型结构计算出`Loss`，使用`SGD`或者其他优化器不断的优化，`PARL`这种架构可以很方便的应用在各类深度强化学习问题中。\n",
    "\n",
    "#### （1）Model\n",
    "`Model`用来定义前向(`Forward`)网络，用户可以自由的定制自己的网络结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Model(parl.Model):\n",
    "    def __init__(self, act_dim, hid_size, activation='tanh'):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 2. 请参考课程Demo，配置model结构\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        act_dim = act_dim\n",
    "        hid1_size = act_dim * hid_size\n",
    "\n",
    "        self.fc1 = layers.fc(size=hid1_size, act=activation)\n",
    "        self.fc2 = layers.fc(size=act_dim, act='softmax')\n",
    "\n",
    "    def forward(self, obs):  # 可直接用 model = Model(5); model(obs)调用\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 3. 请参考课程Demo，组装policy网络\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        out = self.fc1(obs)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （2）Algorithm\n",
    "* `Algorithm` 定义了具体的算法来更新前向网络(`Model`)，也就是通过定义损失函数来更新`Model`，和算法相关的计算都放在`algorithm`中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from parl.algorithms import PolicyGradient # 直接从parl库中导入PolicyGradient算法，无需重复写算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （3）Agent\n",
    "* `Agent`负责算法与环境的交互，在交互过程中把生成的数据提供给`Algorithm`来更新模型(`Model`)，数据的预处理流程也一般定义在这里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Agent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(Agent, self).__init__(algorithm)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):  # 搭建计算图用于 预测动作，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.act_prob = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(\n",
    "                self.learn_program):  # 搭建计算图用于 更新policy网络，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(name='act', shape=[1], dtype='int64')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            self.cost = self.alg.learn(obs, act, reward)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)  # 增加一维维度\n",
    "        act_prob = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.act_prob])[0]\n",
    "        act_prob = np.squeeze(act_prob, axis=0)  # 减少一维维度\n",
    "        act = np.random.choice(range(self.act_dim), p=act_prob)  # 根据动作概率选取动作\n",
    "        return act\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act_prob = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.act_prob])[0]\n",
    "        act_prob = np.squeeze(act_prob, axis=0)\n",
    "        act = np.argmax(act_prob)  # 根据动作概率选择概率最高的动作\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward):\n",
    "        act = np.expand_dims(act, axis=-1)\n",
    "        feed = {\n",
    "            'obs': obs.astype('float32'),\n",
    "            'act': act.astype('int64'),\n",
    "            'reward': reward.astype('float32')\n",
    "        }\n",
    "        cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.cost])[0]\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Training && Test（训练&&测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_episode(env, agent):\n",
    "    obs_list, action_list, reward_list = [], [], []\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs = preprocess(obs) # from shape (210, 160, 3) to (100800,)\n",
    "        obs_list.append(obs)\n",
    "        action = agent.sample(obs) # 采样动作\n",
    "        action_list.append(action)\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward_list.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return obs_list, action_list, reward_list\n",
    "\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，求平均\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            obs = preprocess(obs) # from shape (210, 160, 3) to (100800,)\n",
    "            action = agent.predict(obs) # 选取最优动作\n",
    "            obs, reward, isOver, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if isOver:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6 创建环境和Agent，启动训练，保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Pong 图片预处理\n",
    "def preprocess(image):\n",
    "    \"\"\" 预处理 210x160x3 uint8 frame into 6400 (80x80) 1维 float vector \"\"\"\n",
    "    image = image[35:195]  # 裁剪\n",
    "    image = image[::2, ::2, 0]  # 下采样，缩放2倍\n",
    "    image[image == 144] = 0  # 擦除背景 (background type 1)\n",
    "    image[image == 109] = 0  # 擦除背景 (background type 2)\n",
    "    image[image != 0] = 1  # 转为灰度图，除了黑色外其他都是白色\n",
    "    return image.astype(np.float).ravel()\n",
    "\n",
    "\n",
    "# 根据一个episode的每个step的reward列表，计算每一个Step的Gt\n",
    "def calc_reward_to_go(reward_list, gamma=0.99):\n",
    "    \"\"\"calculate discounted reward\"\"\"\n",
    "    reward_arr = np.array(reward_list)\n",
    "    for i in range(len(reward_arr) - 2, -1, -1):\n",
    "        # G_t = r_t + γ·r_t+1 + ... = r_t + γ·G_t+1\n",
    "        reward_arr[i] += gamma * reward_arr[i + 1]\n",
    "    # normalize episode rewards\n",
    "    reward_arr -= np.mean(reward_arr)\n",
    "    reward_arr /= np.std(reward_arr)\n",
    "    return reward_arr\n",
    "\n",
    "\n",
    "def main(LEARNING_RATE, hid_size, activation, render=False):\n",
    "    # 创建环境\n",
    "    env = gym.make('Pong-v0')\n",
    "    obs_dim = 80 * 80\n",
    "    act_dim = env.action_space.n\n",
    "    logger.info('obs_dim {}, act_dim {}, LEARNING_RATE {}, hid_size {}, activation {}, Pid {}'.format(\n",
    "        obs_dim, act_dim, LEARNING_RATE, hid_size, activation, os.getpid()))\n",
    "\n",
    "    # 根据parl框架构建agent\n",
    "    ######################################################################\n",
    "    ######################################################################\n",
    "    #\n",
    "    # 4. 请参考课堂Demo构建 agent，嵌套Model, PolicyGradient, Agent\n",
    "    #\n",
    "    ######################################################################\n",
    "    ######################################################################\n",
    "    model = Model(act_dim=act_dim, hid_size=hid_size, activation=activation)\n",
    "    alg = PolicyGradient(model, lr=LEARNING_RATE)\n",
    "    agent = Agent(alg, obs_dim=obs_dim, act_dim=act_dim)\n",
    "\n",
    "    # 加载模型\n",
    "    if os.path.exists(f'./model_best_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt'):\n",
    "        agent.restore(f'./model_best_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt')\n",
    "        logger.info(f'Successfully loaded model ./model_best_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt')\n",
    "\n",
    "    best_reward = -100\n",
    "    for i in range(3000):\n",
    "        obs_list, action_list, reward_list = run_episode(env, agent)\n",
    "        if 100 < i < 2900:\n",
    "            if i % 100 == 0:\n",
    "                logger.info(\"Train Episode {}, Reward Sum {}. Pid {}\".format(i, sum(reward_list), os.getpid()))\n",
    "        elif i % 10 == 0:\n",
    "            logger.info(\"Train Episode {}, Reward Sum {}. Pid {}\".format(i, sum(reward_list), os.getpid()))\n",
    "\n",
    "        batch_obs = np.array(obs_list)\n",
    "        batch_action = np.array(action_list)\n",
    "        batch_reward = calc_reward_to_go(reward_list)\n",
    "\n",
    "        agent.learn(batch_obs, batch_action, batch_reward)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            total_reward = evaluate(env, agent, render=render)\n",
    "            logger.info('Episode {}, Test reward: {}, Pid {}'.format(i + 1, total_reward, os.getpid()))\n",
    "            if total_reward > best_reward:\n",
    "                best_reward = total_reward\n",
    "                agent.save(f'./model_best_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt')\n",
    "\n",
    "    # save the parameters to ./model.ckpt\n",
    "    agent.save(f'./model_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def parallel():\n",
    "    # only work in CPU mode\n",
    "    from multiprocessing import Pool\n",
    "    print('Parent process %s.' % os.getpid())\n",
    "    p = Pool(4)\n",
    "    for lr in [0.0015, 0.0025]:\n",
    "        for hs in [10]:\n",
    "            for act in ['tanh', 'relu']:\n",
    "                p.apply_async(main, args=(lr, hs, act))\n",
    "    print('Waiting for all subprocesses done...')\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print('All subprocesses done.')\n",
    "\n",
    "\n",
    "def one(lr, hs, act):\n",
    "    # lr = 0.002\n",
    "    # hs = 20\n",
    "    # act = 'relu'\n",
    "    main(lr, hs, act, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-20 16:15:51 MainThread @<ipython-input-10-486e4fc4cd66>:31]\u001b[0m obs_dim 6400, act_dim 6, LEARNING_RATE 0.004, hid_size 20, activation relu, Pid 60\n",
      "\u001b[32m[06-20 16:15:51 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-20 16:15:51 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-20 16:15:51 MainThread @<ipython-input-10-486e4fc4cd66>:48]\u001b[0m Successfully loaded model ./model_best_lr_0.004_hids_20_relu.ckpt\n",
      "\u001b[32m[06-20 16:16:00 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 0, Reward Sum 6.0. Pid 60\n",
      "\u001b[32m[06-20 16:17:31 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 10, Reward Sum 15.0. Pid 60\n",
      "\u001b[32m[06-20 16:19:01 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 20, Reward Sum 9.0. Pid 60\n",
      "\u001b[32m[06-20 16:20:33 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 30, Reward Sum -4.0. Pid 60\n",
      "\u001b[32m[06-20 16:22:02 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 40, Reward Sum 6.0. Pid 60\n",
      "\u001b[32m[06-20 16:23:36 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 50, Reward Sum -5.0. Pid 60\n",
      "\u001b[32m[06-20 16:25:09 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 60, Reward Sum 3.0. Pid 60\n",
      "\u001b[32m[06-20 16:26:49 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 70, Reward Sum 6.0. Pid 60\n",
      "\u001b[32m[06-20 16:28:26 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 80, Reward Sum -10.0. Pid 60\n",
      "\u001b[32m[06-20 16:30:05 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 90, Reward Sum 2.0. Pid 60\n",
      "\u001b[32m[06-20 16:32:12 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 100, Test reward: 5.2, Pid 60\n",
      "\u001b[32m[06-20 16:32:23 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 100, Reward Sum 2.0. Pid 60\n",
      "\u001b[32m[06-20 16:49:00 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 200, Test reward: -1.0, Pid 60\n",
      "\u001b[32m[06-20 16:49:09 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 200, Reward Sum -2.0. Pid 60\n",
      "\u001b[32m[06-20 17:04:55 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 300, Test reward: 11.0, Pid 60\n",
      "\u001b[32m[06-20 17:05:04 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 300, Reward Sum 7.0. Pid 60\n",
      "\u001b[32m[06-20 17:20:21 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 400, Test reward: 8.0, Pid 60\n",
      "\u001b[32m[06-20 17:20:30 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 400, Reward Sum 7.0. Pid 60\n",
      "\u001b[32m[06-20 17:35:11 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 500, Test reward: 5.6, Pid 60\n",
      "\u001b[32m[06-20 17:35:20 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 500, Reward Sum 11.0. Pid 60\n",
      "\u001b[32m[06-20 17:50:42 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 600, Test reward: 5.0, Pid 60\n",
      "\u001b[32m[06-20 17:50:49 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 600, Reward Sum 15.0. Pid 60\n",
      "\u001b[32m[06-20 18:05:30 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 700, Test reward: 11.0, Pid 60\n",
      "\u001b[32m[06-20 18:05:38 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 700, Reward Sum 15.0. Pid 60\n",
      "\u001b[32m[06-20 18:20:42 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 800, Test reward: 8.0, Pid 60\n",
      "\u001b[32m[06-20 18:20:50 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 800, Reward Sum 12.0. Pid 60\n",
      "\u001b[32m[06-20 18:35:50 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 900, Test reward: 7.4, Pid 60\n",
      "\u001b[32m[06-20 18:36:00 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 900, Reward Sum 6.0. Pid 60\n",
      "\u001b[32m[06-20 18:50:59 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1000, Test reward: 9.6, Pid 60\n",
      "\u001b[32m[06-20 18:51:07 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1000, Reward Sum 13.0. Pid 60\n",
      "\u001b[32m[06-20 19:06:19 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1100, Test reward: 10.6, Pid 60\n",
      "\u001b[32m[06-20 19:06:28 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1100, Reward Sum 8.0. Pid 60\n",
      "\u001b[32m[06-20 19:21:22 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1200, Test reward: 6.2, Pid 60\n",
      "\u001b[32m[06-20 19:21:32 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1200, Reward Sum 3.0. Pid 60\n",
      "\u001b[32m[06-20 19:36:33 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1300, Test reward: 11.6, Pid 60\n",
      "\u001b[32m[06-20 19:36:40 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1300, Reward Sum 16.0. Pid 60\n",
      "\u001b[32m[06-20 19:49:52 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1400, Test reward: -18.4, Pid 60\n",
      "\u001b[32m[06-20 19:49:55 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1400, Reward Sum -20.0. Pid 60\n",
      "\u001b[32m[06-20 20:02:45 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1500, Test reward: 11.2, Pid 60\n",
      "\u001b[32m[06-20 20:02:55 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1500, Reward Sum -1.0. Pid 60\n",
      "\u001b[32m[06-20 20:17:16 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1600, Test reward: 12.4, Pid 60\n",
      "\u001b[32m[06-20 20:17:22 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1600, Reward Sum 19.0. Pid 60\n",
      "\u001b[32m[06-20 20:32:04 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1700, Test reward: 11.2, Pid 60\n",
      "\u001b[32m[06-20 20:32:13 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1700, Reward Sum 7.0. Pid 60\n",
      "\u001b[32m[06-20 20:46:38 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1800, Test reward: 8.6, Pid 60\n",
      "\u001b[32m[06-20 20:46:47 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1800, Reward Sum 11.0. Pid 60\n",
      "\u001b[32m[06-20 21:00:51 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 1900, Test reward: 5.8, Pid 60\n",
      "\u001b[32m[06-20 21:00:59 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 1900, Reward Sum 3.0. Pid 60\n",
      "\u001b[32m[06-20 21:14:55 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2000, Test reward: 12.6, Pid 60\n",
      "\u001b[32m[06-20 21:15:02 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2000, Reward Sum 12.0. Pid 60\n",
      "\u001b[32m[06-20 21:29:27 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2100, Test reward: 15.6, Pid 60\n",
      "\u001b[32m[06-20 21:29:34 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2100, Reward Sum 12.0. Pid 60\n",
      "\u001b[32m[06-20 21:43:44 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2200, Test reward: 14.4, Pid 60\n",
      "\u001b[32m[06-20 21:43:51 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2200, Reward Sum 17.0. Pid 60\n",
      "\u001b[32m[06-20 21:57:31 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2300, Test reward: 13.6, Pid 60\n",
      "\u001b[32m[06-20 21:57:39 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2300, Reward Sum 16.0. Pid 60\n",
      "\u001b[32m[06-20 22:12:03 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2400, Test reward: 14.8, Pid 60\n",
      "\u001b[32m[06-20 22:12:11 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2400, Reward Sum 10.0. Pid 60\n",
      "\u001b[32m[06-20 22:27:08 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2500, Test reward: 14.4, Pid 60\n",
      "\u001b[32m[06-20 22:27:15 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2500, Reward Sum 17.0. Pid 60\n",
      "\u001b[32m[06-20 22:43:02 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2600, Test reward: 6.8, Pid 60\n",
      "\u001b[32m[06-20 22:43:12 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2600, Reward Sum 5.0. Pid 60\n",
      "\u001b[32m[06-20 22:58:34 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2700, Test reward: 12.8, Pid 60\n",
      "\u001b[32m[06-20 22:58:42 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2700, Reward Sum 8.0. Pid 60\n",
      "\u001b[32m[06-20 23:12:27 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2800, Test reward: 1.4, Pid 60\n",
      "\u001b[32m[06-20 23:12:36 MainThread @<ipython-input-10-486e4fc4cd66>:55]\u001b[0m Train Episode 2800, Reward Sum 3.0. Pid 60\n",
      "\u001b[32m[06-20 23:26:21 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 2900, Test reward: 11.6, Pid 60\n",
      "\u001b[32m[06-20 23:26:29 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2900, Reward Sum 12.0. Pid 60\n",
      "\u001b[32m[06-20 23:27:46 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2910, Reward Sum -11.0. Pid 60\n",
      "\u001b[32m[06-20 23:29:02 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2920, Reward Sum 5.0. Pid 60\n",
      "\u001b[32m[06-20 23:30:21 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2930, Reward Sum 13.0. Pid 60\n",
      "\u001b[32m[06-20 23:31:45 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2940, Reward Sum 4.0. Pid 60\n",
      "\u001b[32m[06-20 23:33:06 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2950, Reward Sum 14.0. Pid 60\n",
      "\u001b[32m[06-20 23:34:28 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2960, Reward Sum 14.0. Pid 60\n",
      "\u001b[32m[06-20 23:35:50 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2970, Reward Sum 11.0. Pid 60\n",
      "\u001b[32m[06-20 23:37:07 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2980, Reward Sum 14.0. Pid 60\n",
      "\u001b[32m[06-20 23:38:25 MainThread @<ipython-input-10-486e4fc4cd66>:57]\u001b[0m Train Episode 2990, Reward Sum 17.0. Pid 60\n",
      "\u001b[32m[06-20 23:40:12 MainThread @<ipython-input-10-486e4fc4cd66>:66]\u001b[0m Episode 3000, Test reward: 15.2, Pid 60\n"
     ]
    }
   ],
   "source": [
    "# parallel()\n",
    "one(0.004, 20, 'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-21 00:06:45 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-21 00:06:45 MainThread @machine_info.py:88]\u001b[0m Cannot find available GPU devices, using CPU now.\n",
      "\u001b[32m[06-21 00:07:21 MainThread @<ipython-input-20-401b1b14b375>:25]\u001b[0m obs_dim 6400, act_dim 6, LEARNING_RATE 0.004, hid_size 20, activation relu, gamma 0.99, Test reward: 12.2\n"
     ]
    }
   ],
   "source": [
    "def test(LEARNING_RATE, hid_size, activation, render=False, gamma=0.99):\n",
    "    # LEARNING_RATE = 0.002\n",
    "    # hid_size = 30\n",
    "    # activation = 'sigmoid'\n",
    "    # gamma = 0.9\n",
    "    # render = False\n",
    "\n",
    "    # 创建环境\n",
    "    env = gym.make('Pong-v0')\n",
    "    obs_dim = 80 * 80\n",
    "    act_dim = env.action_space.n\n",
    "    model = Model(act_dim=act_dim, hid_size=hid_size, activation=activation)\n",
    "    alg = PolicyGradient(model, lr=LEARNING_RATE)\n",
    "    agent = Agent(alg, obs_dim=obs_dim, act_dim=act_dim)\n",
    "\n",
    "    # 加载模型\n",
    "    # if os.path.exists(f'./model_best_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt'):\n",
    "    #     agent.restore(f'./model_best_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt')\n",
    "    \n",
    "    if os.path.exists(f'./model_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt'):\n",
    "        agent.restore(f'./model_lr_{LEARNING_RATE}_hids_{hid_size}_{activation}.ckpt')\n",
    "        total_reward = evaluate(env, agent, render=render)\n",
    "        logger.info(\n",
    "            'obs_dim {}, act_dim {}, LEARNING_RATE {}, hid_size {}, activation {}, gamma {}, Test reward: {}'.format(\n",
    "                obs_dim, act_dim, LEARNING_RATE, hid_size, activation, gamma, total_reward))\n",
    "    else:\n",
    "        logger.info(\n",
    "            'No model for obs_dim {}, act_dim {}, LEARNING_RATE {}, hid_size {}, activation {}, gamma {}'.format(\n",
    "                obs_dim, act_dim, LEARNING_RATE, hid_size, activation, gamma))\n",
    "        return -21\n",
    "\n",
    "test(0.004, 20, 'relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "#### 1. 对训练脚本进行了一定优化\n",
    "- 增加多进程并行训练函数，充分利用机器多核，提高训练速度；不过只适用于CPU版，GPU版会出错退出\n",
    "- 增加最优模型保存策略\n",
    "\n",
    "#### 2. 对大量参数组合进行了实验\n",
    "- 模型网络结构方面，实验了隐层为1层和2层fc两种，2层训练太慢，最终选择了1层结构\n",
    "- 对于隐层节点数，实验了10倍act_dim和20倍两种，20倍的效果更好\n",
    "- 对于激活函数，实验了tanh, sigmoid和relu, 发现tanh和relu较好，但relu对学习率的敏感度更低\n",
    "- 对应Gamma，短暂实验了0.9，0.99，0.995，由于一开始的实验结果都不好，后来固定为0.99\n",
    "- 对学习率，实验了[0.001, 0.002, 0.0025, 0.003, 0.004, 0.005, 0.01], 结果 relu + 0.004效果最好\n",
    "\n",
    "#### 3. 训练机器\n",
    "- Baidu AIStudio\n",
    "- Google Colab\n",
    "- MacBook Pro \n",
    "\n",
    "一开始没加入最优模型保存策略，也没提前终止不可行方案，各处攒资源大概用了100 CPU核时才发现本笔记版本中的方案，\n",
    "大概训练了1000Episodes，然后将中间的最优模型上传到这里，继续进行训练，所以现在的模型实际训练了大概4000个Episodes。\n",
    "\n",
    "#### 4. 最终结果\n",
    "- 训练时最后一个Episode的 Test Reward = 15.2\n",
    "- 单独加载最后保存的模型进行多次实验，基本能够稳定使得 Test Reward > 10.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

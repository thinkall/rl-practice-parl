{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlepaddle-gpu              1.8.2.post107\n",
      "parl                          1.3.1\n",
      "rlschool                      0.3.1\n"
     ]
    }
   ],
   "source": [
    "# 检查依赖包版本是否正确\n",
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl\n",
    "!pip list | grep rlschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "from parl.utils import logger\n",
    "from parl.utils import action_mapping # 将神经网络输出映射到对应的 实际动作取值范围内\n",
    "from parl.utils import ReplayMemory # 经验回放\n",
    "from parl.algorithms import DDPG\n",
    "from parl.utils.scheduler import LinearDecayScheduler, PiecewiseScheduler\n",
    "\n",
    "from rlschool import make_env  # 使用 RLSchool 创建飞行器环境\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim, model_tag): \n",
    "        self.model_tag = model_tag\n",
    "        if self.model_tag == 1:\n",
    "            # simple model\n",
    "            hid1_size = 100\n",
    "            hid2_size = 100\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=act_dim, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "        else:\n",
    "            hid1_size = 100\n",
    "            hid2_size = 100\n",
    "            hid3_size = 100\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=hid3_size, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc4 = layers.fc(size=act_dim, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "\n",
    "    def policy(self, obs): \n",
    "        if self.model_tag == 1:\n",
    "            hid = self.fc1(obs)\n",
    "            hid = self.fc2(hid)\n",
    "            logits = self.fc3(hid)\n",
    "        else:                \n",
    "            hid1 = self.fc1(obs)\n",
    "            hid2 = self.fc2(hid1)\n",
    "            hid3 = self.fc3(hid2)\n",
    "            logits = self.fc4(hid3) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(parl.Model):\n",
    "    def __init__(self, model_tag): \n",
    "        self.model_tag = model_tag\n",
    "        if self.model_tag == 1:\n",
    "            hid_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=1, act=None)\n",
    "        else:\n",
    "            hid1_size = 100\n",
    "            hid2_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=1, act=None)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        # 输入 state, action, 输出对应的Q(s,a) \n",
    "        if self.model_tag == 1:\n",
    "            concat = layers.concat([obs, act], axis=1)\n",
    "            hid = self.fc1(concat)\n",
    "            Q = self.fc2(hid)\n",
    "            Q = layers.squeeze(Q, axes=[1])\n",
    "        else:\n",
    "            hid1 = self.fc1(obs)\n",
    "            concat = layers.concat([hid1, act], axis=1)\n",
    "            hid2 = self.fc2(concat)\n",
    "            Q = self.fc3(hid2)\n",
    "            Q = layers.squeeze(Q, axes=[1])\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadrotorModel(parl.Model):\n",
    "    def __init__(self, act_dim, model_tag):\n",
    "        self.model_tag = model_tag\n",
    "        self.actor_model = ActorModel(act_dim, self.model_tag)\n",
    "        self.critic_model = CriticModel(self.model_tag)\n",
    "\n",
    "    def policy(self, obs):\n",
    "        return self.actor_model.policy(obs)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        return self.critic_model.value(obs, act)\n",
    "\n",
    "    def get_actor_params(self):\n",
    "        return self.actor_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadrotorAgent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim=4):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(QuadrotorAgent, self).__init__(algorithm)\n",
    "\n",
    "        # 注意，在最开始的时候，先完全同步target_model和model的参数\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n",
    "                                                 terminal)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        # print(act)\n",
    "# 在rlschool 0.3.0调整会更好，0.3.1无需调整，因为初始状态不是平稳而是随机        \n",
    "#         # 调整输出到均值附近\n",
    "#         act_mean = act.mean(axis=1)\n",
    "#         act = act_mean + (act - act_mean) * 0.1\n",
    "\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        critic_cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n",
    "        self.alg.sync_target()\n",
    "\n",
    "        return critic_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent, rpm):\n",
    "    obs = env.reset()\n",
    "    total_reward, steps = 0, 0\n",
    "    while True:\n",
    "        steps += 1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "        action = np.squeeze(action)\n",
    "\n",
    "        # 给输出动作增加探索扰动，输出限制在 [-1.0, 1.0] 范围内\n",
    "        action = np.clip(np.random.normal(action, 1.0), -1.0, 1.0)\n",
    "        # 动作映射到对应的 实际动作取值范围 内, action_mapping是从parl.utils那里import进来的函数\n",
    "        action = action_mapping(action, env.action_space.low[0],\n",
    "                                env.action_space.high[0])\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        rpm.append(obs, action, REWARD_SCALE * reward, next_obs, done)\n",
    "\n",
    "        if rpm.size() > MEMORY_WARMUP_SIZE:\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, \\\n",
    "                    batch_terminal = rpm.sample_batch(BATCH_SIZE)\n",
    "            critic_cost = agent.learn(batch_obs, batch_action, batch_reward,\n",
    "                                      batch_next_obs, batch_terminal)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, steps\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        total_reward, steps = 0, 0\n",
    "        while True:\n",
    "            batch_obs = np.expand_dims(obs, axis=0)\n",
    "            action = agent.predict(batch_obs.astype('float32'))\n",
    "            action = np.squeeze(action)\n",
    "            action = np.clip(action, -1.0, 1.0)  # the action should be in range [-1.0, 1.0]\n",
    "            action = action_mapping(action, env.action_space.low[0], \n",
    "                                    env.action_space.high[0])\n",
    "\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(total_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_results(score_list, title='', path='./fig_dir/'):\n",
    "    # 画出训练过程reward历史曲线 \n",
    "    plt.plot(score_list, color='green', label='train')\n",
    "#     plt.plot(test_score_list, color='red', label='test')\n",
    "    plt.title('Reward History {}'.format(title))\n",
    "    plt.legend()\n",
    "    if path != '' and not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    plt.savefig(path + title + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=1, load_model=False, go_steps=1, f_best=''):\n",
    "    # 创建飞行器环境\n",
    "    env = make_env(\"Quadrotor\", task=\"velocity_control\") # Yellow arrow is the expected velocity vector; orange arrow is the real velocity vector.\n",
    "    env.reset()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # 根据parl框架构建agent\n",
    "    model = QuadrotorModel(act_dim, model_tag=model_tag)\n",
    "    algorithm = DDPG(model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "    agent = QuadrotorAgent(algorithm, obs_dim, act_dim)\n",
    "\n",
    "    # parl库也为DDPG算法内置了ReplayMemory，可直接从 parl.utils 引入使用\n",
    "    rpm = ReplayMemory(int(MEMORY_SIZE), obs_dim, act_dim)\n",
    "\n",
    "    # 启动训练\n",
    "    logger.info('Params: ACTOR_LR={}, CRITIC_LR={}, model_tag={}, Pid {}'.format(ACTOR_LR, CRITIC_LR, model_tag, os.getpid()))\n",
    "    test_flag = 0\n",
    "    total_steps = 0\n",
    "    early_stop = 0\n",
    "    last_reward = -1e9\n",
    "    best_reward = -1e9\n",
    "    # 与下文结合，使得学习率在训练到90%时保持初始学习率的1%水平继续训练\n",
    "    actor_lr_scheduler = LinearDecayScheduler(ACTOR_LR, int(TRAIN_TOTAL_STEPS*0.9))\n",
    "    critic_lr_scheduler = LinearDecayScheduler(CRITIC_LR, int(TRAIN_TOTAL_STEPS*0.9))\n",
    "    score_list = []\n",
    "    \n",
    "    # load best results and continue training\n",
    "    if load_model == True and os.path.exists(f_best):\n",
    "        agent.restore(f_best + '.ckpt')\n",
    "        rpm.load(f_best + '.rpm.npz')\n",
    "        actor_lr_scheduler.step(step_num=go_steps)\n",
    "        critic_lr_scheduler.step(step_num=go_steps)\n",
    "        logger.info('load model success. Pid {}'.format(os.getpid()))\n",
    "\n",
    "\n",
    "    while total_steps < TRAIN_TOTAL_STEPS:\n",
    "        train_reward, steps = run_episode(env, agent, rpm)\n",
    "        total_steps += steps\n",
    "        #logger.info('Steps: {} Reward: {} Pid: {}'.format(total_steps, train_reward, os.getpid())) # 打印训练reward\n",
    "        score_list.append(train_reward)\n",
    "\n",
    "        # 可以在这里修改学习率, 可以用 parl.utils.scheduler 中的 LinearDecayScheduler 进行修改，也可以自行修改\n",
    "        agent.alg.actor_lr = max(actor_lr_scheduler.step(step_num=steps), ACTOR_LR/100)\n",
    "        agent.alg.critic_lr = max(critic_lr_scheduler.step(step_num=steps), CRITIC_LR/100)\n",
    "\n",
    "        if total_steps // TEST_EVERY_STEPS >= test_flag: # 每隔一定step数，评估一次模型\n",
    "            while total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "                test_flag += 1\n",
    "    \n",
    "            evaluate_reward = evaluate(env, agent)\n",
    "            logger.info('Steps {}, Test reward: {}, Pid {}'.format(total_steps, evaluate_reward, os.getpid())) # 打印评估的reward\n",
    "\n",
    "\n",
    "            # 每评估一次，优于最优模型就保存一次模型和记忆回放，以训练的step数命名，DEBUG时则一直保存模型和图片  \n",
    "            if evaluate_reward > best_reward or DEBUG:  # velocity control task reward will always be negative\n",
    "                ckpt = 'model_dir/steps_{}_evaluate_reward_{}_ACTOR_LR_{}_CRITIC_LR_{}_model_tag_{}'.format(total_steps, int(evaluate_reward), ACTOR_LR, CRITIC_LR, model_tag)\n",
    "                agent.save(ckpt + '.ckpt')\n",
    "                rpm.save(ckpt + '.rpm')\n",
    "                logger.info('Current actor_lr: {}  critic_lr: {}  Pid {} ckpt {}'.format(agent.alg.actor_lr, agent.alg.critic_lr, os.getpid(), ckpt))\n",
    "                \n",
    "                # 每次保存模型时画出当前reward趋势图            \n",
    "                draw_results(score_list, '_'.join([str(ACTOR_LR), str(CRITIC_LR), str(model_tag), str(total_steps)]))\n",
    "            \n",
    "                # update best reward\n",
    "                best_reward = evaluate_reward\n",
    "                \n",
    "            \n",
    "            # early_stop, 超过20%训练进度且连续5次测评reward下降则提前终止\n",
    "            if evaluate_reward > last_reward:\n",
    "                early_stop = 0\n",
    "            else:\n",
    "                early_stop += 1\n",
    "            last_reward = evaluate_reward\n",
    "            if total_steps > TRAIN_TOTAL_STEPS / 5 and early_stop >= 5: \n",
    "                logger.info('No good results, stop training. Params: ACTOR_LR={}, CRITIC_LR={}, model_tag={}, Pid {}'.format(ACTOR_LR, CRITIC_LR, model_tag, os.getpid()))\n",
    "                break\n",
    "    # 训练结束，画出reward趋势图，并保存最终模型\n",
    "    draw_results(score_list, '_'.join([str(ACTOR_LR), str(CRITIC_LR), str(model_tag), str(total_steps)]))\n",
    "    ckpt = 'model_dir/steps_{}_evaluate_reward_{}_ACTOR_LR_{}_CRITIC_LR_{}_model_tag_{}'.format(total_steps, int(evaluate_reward), ACTOR_LR, CRITIC_LR, model_tag)\n",
    "    agent.save(ckpt + '.ckpt')\n",
    "    rpm.save(ckpt + '.rpm')\n",
    "    logger.info('Current actor_lr: {}  critic_lr: {}  Pid {} ckpt {}'.format(agent.alg.actor_lr, agent.alg.critic_lr, os.getpid(), ckpt))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel(num_cores=2, num_gpus=0):\n",
    "    assert isinstance(num_cores, int)\n",
    "    assert num_cores > 0\n",
    "    from multiprocessing import Pool \n",
    "    \n",
    "    # 多进程不能使用GPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "    print('Parent process %s.' % os.getpid()) \n",
    "\n",
    "    p = Pool(num_cores)\n",
    "    used_gpus = 0\n",
    "    for ACTOR_LR in [0.0001, 0.0002, 0.0005, 0.001, 0.002]:  # 0.0002\n",
    "        for CRITIC_LR in [0.001, 0.005, 0.01]:  # 0.001\n",
    "            for model_tag in [1, 2]: # below logs are only for model_tag=2                   \n",
    "                p.apply_async(main, args=(ACTOR_LR, CRITIC_LR, model_tag))\n",
    "\n",
    "    print('Waiting for all subprocesses done...')\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print('All subprocesses done.')\n",
    "\n",
    "\n",
    "def one(ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=2, load_model=False, go_steps=1, f_best='', gpu=''):\n",
    "    # 默认不使用GPU，在我的服务器出现以下错误：\n",
    "    # ExternalError:  Cublas error, CUBLAS_STATUS_NOT_INITIALIZED  at (/paddle/paddle/fluid/platform/cuda_helper.h:32)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "    main(ACTOR_LR, CRITIC_LR, model_tag, load_model, go_steps, f_best)\n",
    "\n",
    "\n",
    "def find_best():\n",
    "    best = -1000000\n",
    "    best_f = ''\n",
    "    for _, _, files in os.walk('./model_dir'):\n",
    "        for f in files:\n",
    "            reward = int(f[f.find('reward')+7:f.find('ACTOR')-1])\n",
    "            if reward >= best:\n",
    "                best = reward\n",
    "                best_f = f\n",
    "    return 'model_dir/' + best_f\n",
    "\n",
    "\n",
    "def fine_tune(ACTOR_LR=0.0002, CRITIC_LR=0.005, episodes=10):\n",
    "    for i in range(episodes):\n",
    "        f_best = find_best()\n",
    "        logger.info('Current best: {}, finetune it...'.format(f_best)) \n",
    "        #todo: extract go_steps from f_best\n",
    "        one(ACTOR_LR=ACTOR_LR, CRITIC_LR=CRITIC_LR, model_tag=2, load_model=True, go_steps=1, f_best=f_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_steps, evaluate_reward, ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=2, render=False):\n",
    "    # 请设置ckpt为你训练中效果最好的一次评估保存的模型文件名称\n",
    "    ckpt = 'model_dir/steps_{}_evaluate_reward_{}_ACTOR_LR_{}_CRITIC_LR_{}_model_tag_{}.ckpt'.format(total_steps, int(evaluate_reward), ACTOR_LR, CRITIC_LR, model_tag)\n",
    "    env = make_env(\"Quadrotor\", task=\"velocity_control\")\n",
    "    env.reset()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    model = QuadrotorModel(act_dim, model_tag=model_tag)\n",
    "    algorithm = DDPG(model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "    agent = QuadrotorAgent(algorithm, obs_dim, act_dim)\n",
    "    # 加载模型\n",
    "    if os.path.exists(ckpt):\n",
    "        agent.restore(ckpt)\n",
    "        logger.info('Test Model file {}'.format(ckpt))\n",
    "    else:\n",
    "        logger.info('No Model file {}'.format(ckpt))\n",
    "        return -1\n",
    "    evaluate_reward = evaluate(env, agent, render=render)\n",
    "    logger.info('Evaluate reward: {}'.format(evaluate_reward)) # 打印评估的reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best():\n",
    "    best = -1000000\n",
    "    best_f = ''\n",
    "    for _, _, files in os.walk('./model_dir'):\n",
    "        for f in files:\n",
    "            reward = int(f[f.find('reward')+7:f.find('ACTOR')-1])\n",
    "            if reward >= best:\n",
    "                best = reward\n",
    "                best_f = f \n",
    "    res = best_f.split('_') \n",
    "    test(res[1], res[4], float(res[7]), float(res[10]), int(res[-1].split('.')[0]), render=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR = 0.0002   # Actor网络更新的 learning rate\n",
    "CRITIC_LR = 0.005   # Critic网络更新的 learning rate\n",
    "\n",
    "GAMMA = 0.99        # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "TAU = 0.001         # target_model 跟 model 同步参数 的 软更新参数\n",
    "MEMORY_SIZE = 2*1e6   # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 2*1e4      # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn\n",
    "REWARD_SCALE = 0.01       # reward 的缩放因子\n",
    "BATCH_SIZE = 512          # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "TRAIN_TOTAL_STEPS = 1e6   # 总训练步数\n",
    "TEST_EVERY_STEPS = 1e4    # 每个N步评估一下算法效果，每次评估5个episode求平均reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent process 1127.\n",
      "Waiting for all subprocesses done...\n",
      "\u001b[32m[06-24 14:58:45 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0001, CRITIC_LR=0.001, model_tag=2, Pid 2091\n",
      "\u001b[32m[06-24 14:58:47 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.002, CRITIC_LR=0.001, model_tag=2, Pid 2104\n",
      "\u001b[32m[06-24 14:58:48 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0005, CRITIC_LR=0.01, model_tag=2, Pid 2100\n",
      "\u001b[32m[06-24 14:58:48 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.01, model_tag=2, Pid 2097\n",
      "\u001b[32m[06-24 14:58:48 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0001, CRITIC_LR=0.005, model_tag=2, Pid 2092\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0005, CRITIC_LR=0.005, model_tag=2, Pid 2099\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.002, CRITIC_LR=0.01, model_tag=2, Pid 2106\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.001, CRITIC_LR=0.01, model_tag=2, Pid 2103\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0005, CRITIC_LR=0.001, model_tag=2, Pid 2098\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=2, Pid 2095\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.001, CRITIC_LR=0.005, model_tag=2, Pid 2102\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0001, CRITIC_LR=0.01, model_tag=2, Pid 2093\n",
      "\u001b[32m[06-24 14:58:49 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.002, CRITIC_LR=0.005, model_tag=2, Pid 2105\n",
      "\u001b[32m[06-24 14:58:50 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 2096\n",
      "\u001b[32m[06-24 14:58:52 MainThread @<ipython-input-9-8259d0fb9623>:17]\u001b[0m Params: ACTOR_LR=0.001, CRITIC_LR=0.001, model_tag=2, Pid 2101\n",
      "\u001b[32m[06-24 15:00:52 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -762.7086458010807, Pid 2091\n",
      "\u001b[32m[06-24 15:00:53 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.98888888888889e-05  critic_lr: 0.000998888888888889  Pid 2091 ckpt model_dir/steps_1000_evaluate_reward_-762_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:01:24 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -766.5122055203005, Pid 2099\n",
      "\u001b[32m[06-24 15:01:25 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -941.6643154642603, Pid 2106\n",
      "\u001b[32m[06-24 15:01:27 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004994444444444445  critic_lr: 0.004994444444444445  Pid 2099 ckpt model_dir/steps_1000_evaluate_reward_-766_ACTOR_LR_0.0005_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:01:27 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.001997777777777778  critic_lr: 0.00998888888888889  Pid 2106 ckpt model_dir/steps_1000_evaluate_reward_-941_ACTOR_LR_0.002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:01:29 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -680.2600959345323, Pid 2100\n",
      "\u001b[32m[06-24 15:01:30 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004994444444444445  critic_lr: 0.00998888888888889  Pid 2100 ckpt model_dir/steps_1000_evaluate_reward_-680_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:01:32 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -843.7070515040534, Pid 2092\n",
      "\u001b[32m[06-24 15:01:32 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -900.3898124775758, Pid 2104\n",
      "\u001b[32m[06-24 15:01:32 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -840.2060416238868, Pid 2097\n",
      "\u001b[32m[06-24 15:01:32 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -800.9485365644746, Pid 2105\n",
      "\u001b[32m[06-24 15:01:33 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -830.2463426659294, Pid 2098\n",
      "\u001b[32m[06-24 15:01:34 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -885.7566006677538, Pid 2102\n",
      "\u001b[32m[06-24 15:01:34 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.98888888888889e-05  critic_lr: 0.004994444444444445  Pid 2092 ckpt model_dir/steps_1000_evaluate_reward_-843_ACTOR_LR_0.0001_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:01:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -922.2786669403098, Pid 2101\n",
      "\u001b[32m[06-24 15:01:35 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0001997777777777778  critic_lr: 0.00998888888888889  Pid 2097 ckpt model_dir/steps_1000_evaluate_reward_-840_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:01:35 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.001997777777777778  critic_lr: 0.000998888888888889  Pid 2104 ckpt model_dir/steps_1000_evaluate_reward_-900_ACTOR_LR_0.002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:01:35 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.001997777777777778  critic_lr: 0.004994444444444445  Pid 2105 ckpt model_dir/steps_1000_evaluate_reward_-800_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:01:36 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -847.0579041794681, Pid 2096\n",
      "\u001b[32m[06-24 15:01:36 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004994444444444445  critic_lr: 0.000998888888888889  Pid 2098 ckpt model_dir/steps_1000_evaluate_reward_-830_ACTOR_LR_0.0005_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:01:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -804.4576841296439, Pid 2095\n",
      "\u001b[32m[06-24 15:01:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -653.0311795344661, Pid 2103\n",
      "\u001b[32m[06-24 15:01:37 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.000998888888888889  critic_lr: 0.004994444444444445  Pid 2102 ckpt model_dir/steps_1000_evaluate_reward_-885_ACTOR_LR_0.001_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:01:37 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.000998888888888889  critic_lr: 0.000998888888888889  Pid 2101 ckpt model_dir/steps_1000_evaluate_reward_-922_ACTOR_LR_0.001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:01:39 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0001997777777777778  critic_lr: 0.004994444444444445  Pid 2096 ckpt model_dir/steps_1000_evaluate_reward_-847_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:01:39 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0001997777777777778  critic_lr: 0.000998888888888889  Pid 2095 ckpt model_dir/steps_1000_evaluate_reward_-804_ACTOR_LR_0.0002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:01:40 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.000998888888888889  critic_lr: 0.00998888888888889  Pid 2103 ckpt model_dir/steps_1000_evaluate_reward_-653_ACTOR_LR_0.001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:01:40 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 1000, Test reward: -519.1999676484987, Pid 2093\n",
      "\u001b[32m[06-24 15:01:42 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.98888888888889e-05  critic_lr: 0.00998888888888889  Pid 2093 ckpt model_dir/steps_1000_evaluate_reward_-519_ACTOR_LR_0.0001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:06:42 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -829.7039667937222, Pid 2091\n",
      "\u001b[32m[06-24 15:08:09 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -868.1842304026031, Pid 2104\n",
      "\u001b[32m[06-24 15:08:11 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.001977777777777778  critic_lr: 0.000988888888888889  Pid 2104 ckpt model_dir/steps_10000_evaluate_reward_-868_ACTOR_LR_0.002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:08:14 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -949.3141882974938, Pid 2106\n",
      "\u001b[32m[06-24 15:08:18 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -898.0350975378062, Pid 2102\n",
      "\u001b[32m[06-24 15:08:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -773.4625228850543, Pid 2105\n",
      "\u001b[32m[06-24 15:08:22 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -826.3466234648619, Pid 2092\n",
      "\u001b[32m[06-24 15:08:22 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.001977777777777778  critic_lr: 0.004944444444444445  Pid 2105 ckpt model_dir/steps_10000_evaluate_reward_-773_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:08:23 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -801.3209230820012, Pid 2099\n",
      "\u001b[32m[06-24 15:08:25 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -841.7500502666387, Pid 2097\n",
      "\u001b[32m[06-24 15:08:25 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.888888888888889e-05  critic_lr: 0.004944444444444445  Pid 2092 ckpt model_dir/steps_10000_evaluate_reward_-826_ACTOR_LR_0.0001_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:08:26 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -677.494369185617, Pid 2100\n",
      "\u001b[32m[06-24 15:08:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -860.1828664224398, Pid 2096\n",
      "\u001b[32m[06-24 15:08:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -897.8482363043165, Pid 2098\n",
      "\u001b[32m[06-24 15:08:28 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004944444444444445  critic_lr: 0.00988888888888889  Pid 2100 ckpt model_dir/steps_10000_evaluate_reward_-677_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:08:33 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -920.4163253843872, Pid 2101\n",
      "\u001b[32m[06-24 15:08:35 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.000988888888888889  critic_lr: 0.000988888888888889  Pid 2101 ckpt model_dir/steps_10000_evaluate_reward_-920_ACTOR_LR_0.001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:08:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -720.2229580906459, Pid 2095\n",
      "\u001b[32m[06-24 15:08:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -668.0705704366111, Pid 2103\n",
      "\u001b[32m[06-24 15:08:38 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00019777777777777778  critic_lr: 0.000988888888888889  Pid 2095 ckpt model_dir/steps_10000_evaluate_reward_-720_ACTOR_LR_0.0002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:08:45 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 10000, Test reward: -509.7343333796759, Pid 2093\n",
      "\u001b[32m[06-24 15:08:47 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.888888888888889e-05  critic_lr: 0.00988888888888889  Pid 2093 ckpt model_dir/steps_10000_evaluate_reward_-509_ACTOR_LR_0.0001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:12:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -852.4363611066034, Pid 2091\n",
      "\u001b[32m[06-24 15:15:53 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -927.0587790989424, Pid 2106\n",
      "\u001b[32m[06-24 15:15:55 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0019555555555555554  critic_lr: 0.009777777777777778  Pid 2106 ckpt model_dir/steps_20000_evaluate_reward_-927_ACTOR_LR_0.002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:15:55 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -848.4732716982977, Pid 2104\n",
      "\u001b[32m[06-24 15:15:57 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0019555555555555554  critic_lr: 0.0009777777777777777  Pid 2104 ckpt model_dir/steps_20000_evaluate_reward_-848_ACTOR_LR_0.002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:16:03 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -769.8033214186833, Pid 2105\n",
      "\u001b[32m[06-24 15:16:04 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -914.1897082734301, Pid 2102\n",
      "\u001b[32m[06-24 15:16:05 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0019555555555555554  critic_lr: 0.004888888888888889  Pid 2105 ckpt model_dir/steps_20000_evaluate_reward_-769_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:16:10 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -692.097737596555, Pid 2099\n",
      "\u001b[32m[06-24 15:16:12 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004888888888888889  critic_lr: 0.004888888888888889  Pid 2099 ckpt model_dir/steps_20000_evaluate_reward_-692_ACTOR_LR_0.0005_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:16:12 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -932.8258274910747, Pid 2101\n",
      "\u001b[32m[06-24 15:16:13 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -828.2736676986908, Pid 2097\n",
      "\u001b[32m[06-24 15:16:16 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00019555555555555556  critic_lr: 0.009777777777777778  Pid 2097 ckpt model_dir/steps_20000_evaluate_reward_-828_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:16:22 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -902.436738839072, Pid 2098\n",
      "\u001b[32m[06-24 15:16:24 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -862.6753677653581, Pid 2096\n",
      "\u001b[32m[06-24 15:16:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -862.382051087164, Pid 2092\n",
      "\u001b[32m[06-24 15:16:31 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -797.7171135101496, Pid 2095\n",
      "\u001b[32m[06-24 15:16:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -693.6478357738445, Pid 2100\n",
      "\u001b[32m[06-24 15:16:36 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -680.7597928664796, Pid 2103\n",
      "\u001b[32m[06-24 15:16:44 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 20000, Test reward: -541.8514523951146, Pid 2093\n",
      "\u001b[32m[06-24 15:25:55 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -890.2888413747776, Pid 2091\n",
      "\u001b[32m[06-24 15:33:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -858.4999948502364, Pid 2104\n",
      "\u001b[32m[06-24 15:33:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -509.01081253508255, Pid 2106\n",
      "\u001b[32m[06-24 15:33:30 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0019333333333333333  critic_lr: 0.009666666666666667  Pid 2106 ckpt model_dir/steps_30000_evaluate_reward_-509_ACTOR_LR_0.002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:33:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -576.6058556871847, Pid 2105\n",
      "\u001b[32m[06-24 15:33:43 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0019333333333333333  critic_lr: 0.004833333333333334  Pid 2105 ckpt model_dir/steps_30000_evaluate_reward_-576_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:33:49 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -464.19856964356495, Pid 2102\n",
      "\u001b[32m[06-24 15:33:49 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -657.0272780072431, Pid 2099\n",
      "\u001b[32m[06-24 15:33:51 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0009666666666666667  critic_lr: 0.004833333333333334  Pid 2102 ckpt model_dir/steps_30000_evaluate_reward_-464_ACTOR_LR_0.001_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:33:52 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00048333333333333334  critic_lr: 0.004833333333333334  Pid 2099 ckpt model_dir/steps_30000_evaluate_reward_-657_ACTOR_LR_0.0005_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:33:56 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -771.02991659097, Pid 2101\n",
      "\u001b[32m[06-24 15:33:58 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0009666666666666667  critic_lr: 0.0009666666666666667  Pid 2101 ckpt model_dir/steps_30000_evaluate_reward_-771_ACTOR_LR_0.001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:34:02 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -760.230832406833, Pid 2097\n",
      "\u001b[32m[06-24 15:34:04 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00019333333333333333  critic_lr: 0.009666666666666667  Pid 2097 ckpt model_dir/steps_30000_evaluate_reward_-760_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:34:09 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -689.9897611630273, Pid 2096\n",
      "\u001b[32m[06-24 15:34:11 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -777.6826717421169, Pid 2095\n",
      "\u001b[32m[06-24 15:34:11 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00019333333333333333  critic_lr: 0.004833333333333334  Pid 2096 ckpt model_dir/steps_30000_evaluate_reward_-689_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:34:11 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -524.5911154616996, Pid 2092\n",
      "\u001b[32m[06-24 15:34:13 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.666666666666667e-05  critic_lr: 0.004833333333333334  Pid 2092 ckpt model_dir/steps_30000_evaluate_reward_-524_ACTOR_LR_0.0001_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:34:19 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -881.6147550656751, Pid 2098\n",
      "\u001b[32m[06-24 15:34:42 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -557.4574745894386, Pid 2093\n",
      "\u001b[32m[06-24 15:34:47 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -685.3616966684324, Pid 2103\n",
      "\u001b[32m[06-24 15:34:48 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 30000, Test reward: -747.4264559558685, Pid 2100\n",
      "\u001b[32m[06-24 15:40:14 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -944.8511112290838, Pid 2091\n",
      "\u001b[32m[06-24 15:50:47 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -695.1885631074323, Pid 2105\n",
      "\u001b[32m[06-24 15:51:02 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -598.793362002765, Pid 2106\n",
      "\u001b[32m[06-24 15:51:05 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -955.4953827999943, Pid 2104\n",
      "\u001b[32m[06-24 15:51:15 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -539.1259610025743, Pid 2099\n",
      "\u001b[32m[06-24 15:51:17 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004777777777777778  critic_lr: 0.004777777777777778  Pid 2099 ckpt model_dir/steps_40000_evaluate_reward_-539_ACTOR_LR_0.0005_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:51:18 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -816.2691880885809, Pid 2101\n",
      "\u001b[32m[06-24 15:51:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -686.1151856672543, Pid 2096\n",
      "\u001b[32m[06-24 15:51:29 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00019111111111111114  critic_lr: 0.004777777777777778  Pid 2096 ckpt model_dir/steps_40000_evaluate_reward_-686_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 15:51:39 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -882.2550863909585, Pid 2097\n",
      "\u001b[32m[06-24 15:51:48 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -652.1932596982039, Pid 2102\n",
      "\u001b[32m[06-24 15:51:58 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -754.5447021168486, Pid 2092\n",
      "\u001b[32m[06-24 15:52:03 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -785.5712001335714, Pid 2098\n",
      "\u001b[32m[06-24 15:52:05 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004777777777777778  critic_lr: 0.0009555555555555556  Pid 2098 ckpt model_dir/steps_40000_evaluate_reward_-785_ACTOR_LR_0.0005_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 15:52:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -771.3250268917828, Pid 2095\n",
      "\u001b[32m[06-24 15:52:26 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -260.4299800968421, Pid 2093\n",
      "\u001b[32m[06-24 15:52:28 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.555555555555557e-05  critic_lr: 0.009555555555555557  Pid 2093 ckpt model_dir/steps_40000_evaluate_reward_-260_ACTOR_LR_0.0001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:52:32 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -273.06519622128457, Pid 2103\n",
      "\u001b[32m[06-24 15:52:35 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0009555555555555556  critic_lr: 0.009555555555555557  Pid 2103 ckpt model_dir/steps_40000_evaluate_reward_-273_ACTOR_LR_0.001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:52:52 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 40000, Test reward: -461.76779620291126, Pid 2100\n",
      "\u001b[32m[06-24 15:52:54 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004777777777777778  critic_lr: 0.009555555555555557  Pid 2100 ckpt model_dir/steps_40000_evaluate_reward_-461_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 15:53:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -784.0590945328481, Pid 2091\n",
      "\u001b[32m[06-24 16:07:03 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -855.8486625998139, Pid 2091\n",
      "\u001b[32m[06-24 16:07:55 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -540.2244730143839, Pid 2105\n",
      "\u001b[32m[06-24 16:07:57 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.001888888888888889  critic_lr: 0.004722222222222222  Pid 2105 ckpt model_dir/steps_50000_evaluate_reward_-540_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 16:08:30 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -882.7965189097771, Pid 2104\n",
      "\u001b[32m[06-24 16:08:51 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -883.6354612643333, Pid 2106\n",
      "\u001b[32m[06-24 16:08:54 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -862.9049315271055, Pid 2099\n",
      "\u001b[32m[06-24 16:09:00 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -869.8852213438137, Pid 2101\n",
      "\u001b[32m[06-24 16:09:13 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -798.8096455294847, Pid 2096\n",
      "\u001b[32m[06-24 16:09:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -584.9683622575216, Pid 2097\n",
      "\u001b[32m[06-24 16:09:29 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00018888888888888888  critic_lr: 0.009444444444444445  Pid 2097 ckpt model_dir/steps_50000_evaluate_reward_-584_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 16:09:57 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -916.7415120653299, Pid 2098\n",
      "\u001b[32m[06-24 16:09:58 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -821.42949181893, Pid 2095\n",
      "\u001b[32m[06-24 16:10:02 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -935.2841486807787, Pid 2092\n",
      "\u001b[32m[06-24 16:10:08 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -824.835657414271, Pid 2102\n",
      "\u001b[32m[06-24 16:10:34 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -510.51081087276987, Pid 2093\n",
      "\u001b[32m[06-24 16:10:36 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -884.5939934631112, Pid 2103\n",
      "\u001b[32m[06-24 16:10:53 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 50000, Test reward: -669.0586035788472, Pid 2100\n",
      "\u001b[32m[06-24 16:19:51 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -799.8168379567369, Pid 2091\n",
      "\u001b[32m[06-24 16:25:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -678.8784839073105, Pid 2105\n",
      "\u001b[32m[06-24 16:26:26 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -937.352281764585, Pid 2104\n",
      "\u001b[32m[06-24 16:26:49 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -720.8104432013046, Pid 2099\n",
      "\u001b[32m[06-24 16:27:01 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -764.7685992280873, Pid 2106\n",
      "\u001b[32m[06-24 16:27:23 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -921.6915096610344, Pid 2096\n",
      "\u001b[32m[06-24 16:27:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -935.7471628235517, Pid 2101\n",
      "\u001b[32m[06-24 16:27:50 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -876.4933054728274, Pid 2097\n",
      "\u001b[32m[06-24 16:28:17 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -890.5405443324751, Pid 2095\n",
      "\u001b[32m[06-24 16:28:18 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -929.1211035413622, Pid 2098\n",
      "\u001b[32m[06-24 16:28:26 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -888.682995937, Pid 2102\n",
      "\u001b[32m[06-24 16:28:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -704.9237038406357, Pid 2092\n",
      "\u001b[32m[06-24 16:29:12 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -483.06654166755254, Pid 2093\n",
      "\u001b[32m[06-24 16:29:17 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -795.8799738783309, Pid 2103\n",
      "\u001b[32m[06-24 16:29:21 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 60000, Test reward: -653.3268374988209, Pid 2100\n",
      "\u001b[32m[06-24 16:33:05 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -912.232263850097, Pid 2091\n",
      "\u001b[32m[06-24 16:43:59 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -856.6493377678413, Pid 2105\n",
      "\u001b[32m[06-24 16:44:15 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -911.2282340284968, Pid 2104\n",
      "\u001b[32m[06-24 16:44:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -858.3153157685286, Pid 2099\n",
      "\u001b[32m[06-24 16:44:50 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -916.3884415020957, Pid 2091\n",
      "\u001b[32m[06-24 16:45:29 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -826.0364247710082, Pid 2096\n",
      "\u001b[32m[06-24 16:45:31 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -798.7393327311459, Pid 2106\n",
      "\u001b[32m[06-24 16:46:08 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -884.3712535008281, Pid 2101\n",
      "\u001b[32m[06-24 16:46:18 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -836.6073937911975, Pid 2097\n",
      "\u001b[32m[06-24 16:46:39 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -689.6749939805948, Pid 2095\n",
      "\u001b[32m[06-24 16:46:41 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00018444444444444446  critic_lr: 0.0009222222222222223  Pid 2095 ckpt model_dir/steps_70000_evaluate_reward_-689_ACTOR_LR_0.0002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 16:46:53 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -798.9763759521686, Pid 2098\n",
      "\u001b[32m[06-24 16:46:57 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -785.8720869697433, Pid 2092\n",
      "\u001b[32m[06-24 16:47:12 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -838.1061319732895, Pid 2102\n",
      "\u001b[32m[06-24 16:47:50 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -350.209262853808, Pid 2093\n",
      "\u001b[32m[06-24 16:47:51 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -633.0261910149792, Pid 2100\n",
      "\u001b[32m[06-24 16:47:53 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 70000, Test reward: -920.153085186106, Pid 2103\n",
      "\u001b[32m[06-24 16:57:24 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -662.8122111417573, Pid 2091\n",
      "\u001b[32m[06-24 16:57:25 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 8.888888888888889e-05  critic_lr: 0.0008888888888888888  Pid 2091 ckpt model_dir/steps_100000_evaluate_reward_-662_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 17:02:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -901.169624414569, Pid 2104\n",
      "\u001b[32m[06-24 17:02:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -768.645119655687, Pid 2099\n",
      "\u001b[32m[06-24 17:02:50 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -827.2142582792985, Pid 2105\n",
      "\u001b[32m[06-24 17:04:10 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -738.1165102941261, Pid 2096\n",
      "\u001b[32m[06-24 17:04:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -799.5425427973249, Pid 2106\n",
      "\u001b[32m[06-24 17:05:00 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -917.3214903924197, Pid 2101\n",
      "\u001b[32m[06-24 17:05:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -630.4009314448213, Pid 2097\n",
      "\u001b[32m[06-24 17:05:12 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -765.8949935571754, Pid 2095\n",
      "\u001b[32m[06-24 17:05:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -894.8913084517659, Pid 2098\n",
      "\u001b[32m[06-24 17:05:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -520.6481105784419, Pid 2092\n",
      "\u001b[32m[06-24 17:05:30 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9.111111111111112e-05  critic_lr: 0.004555555555555556  Pid 2092 ckpt model_dir/steps_80000_evaluate_reward_-520_ACTOR_LR_0.0001_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 17:05:54 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -796.7777094564835, Pid 2102\n",
      "\u001b[32m[06-24 17:06:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -802.4616679918988, Pid 2100\n",
      "\u001b[32m[06-24 17:06:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -745.4758198557186, Pid 2093\n",
      "\u001b[32m[06-24 17:06:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 80000, Test reward: -758.5877150619137, Pid 2103\n",
      "\u001b[32m[06-24 17:07:38 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -867.1186427192512, Pid 2091\n",
      "\u001b[32m[06-24 17:19:43 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -850.6192040495522, Pid 2091\n",
      "\u001b[32m[06-24 17:19:52 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -970.1818023374022, Pid 2104\n",
      "\u001b[32m[06-24 17:20:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -810.0404194908958, Pid 2099\n",
      "\u001b[32m[06-24 17:21:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -582.8429546241479, Pid 2105\n",
      "\u001b[32m[06-24 17:22:36 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -816.1373117826415, Pid 2096\n",
      "\u001b[32m[06-24 17:23:02 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -690.4119620565909, Pid 2106\n",
      "\u001b[32m[06-24 17:23:15 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -964.3171087306615, Pid 2101\n",
      "\u001b[32m[06-24 17:23:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -929.4595837058538, Pid 2098\n",
      "\u001b[32m[06-24 17:23:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -494.6992588610284, Pid 2097\n",
      "\u001b[32m[06-24 17:23:31 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00018  critic_lr: 0.009000000000000001  Pid 2097 ckpt model_dir/steps_90000_evaluate_reward_-494_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 17:23:44 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -825.6783160519396, Pid 2095\n",
      "\u001b[32m[06-24 17:23:47 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -380.13947358435496, Pid 2092\n",
      "\u001b[32m[06-24 17:23:50 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 9e-05  critic_lr: 0.0045000000000000005  Pid 2092 ckpt model_dir/steps_90000_evaluate_reward_-380_ACTOR_LR_0.0001_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 17:24:09 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -755.9718940595834, Pid 2102\n",
      "\u001b[32m[06-24 17:24:40 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -422.39817334121255, Pid 2100\n",
      "\u001b[32m[06-24 17:24:42 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00045000000000000004  critic_lr: 0.009000000000000001  Pid 2100 ckpt model_dir/steps_90000_evaluate_reward_-422_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 17:25:04 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -798.2996329921282, Pid 2093\n",
      "\u001b[32m[06-24 17:25:07 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 90000, Test reward: -731.1892298970728, Pid 2103\n",
      "\u001b[32m[06-24 17:31:08 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -813.0204828788839, Pid 2091\n",
      "\u001b[32m[06-24 17:41:55 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -951.4993555874877, Pid 2104\n",
      "\u001b[32m[06-24 17:43:24 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -703.3932887099932, Pid 2099\n",
      "\u001b[32m[06-24 17:46:42 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -595.6705648123083, Pid 2105\n",
      "\u001b[32m[06-24 17:48:42 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -803.5034029540665, Pid 2096\n",
      "\u001b[32m[06-24 17:49:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -974.1616942648861, Pid 2106\n",
      "\u001b[32m[06-24 17:50:15 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -863.8809098347723, Pid 2098\n",
      "\u001b[32m[06-24 17:50:24 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -888.0811910114586, Pid 2101\n",
      "\u001b[32m[06-24 17:50:33 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -866.1568347984497, Pid 2091\n",
      "\u001b[32m[06-24 17:50:55 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -571.4642722085257, Pid 2097\n",
      "\u001b[32m[06-24 17:51:08 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -683.4774193341627, Pid 2095\n",
      "\u001b[32m[06-24 17:51:12 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00017777777777777779  critic_lr: 0.0008888888888888888  Pid 2095 ckpt model_dir/steps_100000_evaluate_reward_-683_ACTOR_LR_0.0002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 17:51:21 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -816.1267400225661, Pid 2092\n",
      "\u001b[32m[06-24 17:52:18 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -864.0329714431439, Pid 2102\n",
      "\u001b[32m[06-24 17:52:52 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -633.4712543641535, Pid 2100\n",
      "\u001b[32m[06-24 17:53:50 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -666.8159282912818, Pid 2093\n",
      "\u001b[32m[06-24 17:54:25 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 100000, Test reward: -749.2964560005934, Pid 2103\n",
      "\u001b[32m[06-24 18:12:25 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -899.8092394086783, Pid 2091\n",
      "\u001b[32m[06-24 18:17:31 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -958.0782360850728, Pid 2104\n",
      "\u001b[32m[06-24 18:19:30 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -928.9854673404977, Pid 2099\n",
      "\u001b[32m[06-24 18:22:17 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -496.47737323450656, Pid 2105\n",
      "\u001b[32m[06-24 18:22:21 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0017555555555555556  critic_lr: 0.004388888888888889  Pid 2105 ckpt model_dir/steps_110000_evaluate_reward_-496_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 18:24:59 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -726.8645878579358, Pid 2096\n",
      "\u001b[32m[06-24 18:25:47 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -895.7200754878064, Pid 2098\n",
      "\u001b[32m[06-24 18:25:56 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -925.6359029232859, Pid 2106\n",
      "\u001b[32m[06-24 18:26:10 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -554.8168573027413, Pid 2097\n",
      "\u001b[32m[06-24 18:26:18 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -986.3877480042092, Pid 2101\n",
      "\u001b[32m[06-24 18:27:00 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -933.7348020217001, Pid 2092\n",
      "\u001b[32m[06-24 18:27:01 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -435.7316510758016, Pid 2095\n",
      "\u001b[32m[06-24 18:27:05 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00017555555555555556  critic_lr: 0.0008777777777777778  Pid 2095 ckpt model_dir/steps_110000_evaluate_reward_-435_ACTOR_LR_0.0002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 18:28:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -774.1548889926651, Pid 2102\n",
      "\u001b[32m[06-24 18:29:07 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -207.97862653725844, Pid 2100\n",
      "\u001b[32m[06-24 18:29:11 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004388888888888889  critic_lr: 0.008777777777777778  Pid 2100 ckpt model_dir/steps_110000_evaluate_reward_-207_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 18:30:01 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -254.97863692843353, Pid 2093\n",
      "\u001b[32m[06-24 18:30:04 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 8.777777777777778e-05  critic_lr: 0.008777777777777778  Pid 2093 ckpt model_dir/steps_110000_evaluate_reward_-254_ACTOR_LR_0.0001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 18:30:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 110000, Test reward: -729.8598614664995, Pid 2103\n",
      "\u001b[32m[06-24 18:34:24 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -810.002053436185, Pid 2091\n",
      "\u001b[32m[06-24 18:53:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -955.4310707364746, Pid 2104\n",
      "\u001b[32m[06-24 18:54:48 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -913.8099457078333, Pid 2099\n",
      "\u001b[32m[06-24 18:55:42 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -834.2204623337742, Pid 2091\n",
      "\u001b[32m[06-24 18:57:42 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -338.4319056559752, Pid 2105\n",
      "\u001b[32m[06-24 18:57:45 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0017333333333333335  critic_lr: 0.004333333333333334  Pid 2105 ckpt model_dir/steps_120000_evaluate_reward_-338_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 19:00:40 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -639.4515313303499, Pid 2096\n",
      "\u001b[32m[06-24 19:00:44 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00017333333333333334  critic_lr: 0.004333333333333334  Pid 2096 ckpt model_dir/steps_120000_evaluate_reward_-639_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 19:01:01 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -246.91245895796334, Pid 2106\n",
      "\u001b[32m[06-24 19:01:04 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -944.1501083634869, Pid 2098\n",
      "\u001b[32m[06-24 19:01:06 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0017333333333333335  critic_lr: 0.008666666666666668  Pid 2106 ckpt model_dir/steps_120000_evaluate_reward_-246_ACTOR_LR_0.002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 19:01:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -464.2546209111553, Pid 2097\n",
      "\u001b[32m[06-24 19:01:10 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00017333333333333334  critic_lr: 0.008666666666666668  Pid 2097 ckpt model_dir/steps_120000_evaluate_reward_-464_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 19:01:30 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -955.4589610421401, Pid 2101\n",
      "\u001b[32m[06-24 19:02:07 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -892.8205813511695, Pid 2092\n",
      "\u001b[32m[06-24 19:02:45 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -342.60160919119335, Pid 2095\n",
      "\u001b[32m[06-24 19:02:48 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00017333333333333334  critic_lr: 0.0008666666666666667  Pid 2095 ckpt model_dir/steps_120000_evaluate_reward_-342_ACTOR_LR_0.0002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 19:04:08 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -277.1561430894808, Pid 2100\n",
      "\u001b[32m[06-24 19:04:55 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -733.3370532307013, Pid 2102\n",
      "\u001b[32m[06-24 19:05:16 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -492.75043789458186, Pid 2093\n",
      "\u001b[32m[06-24 19:06:10 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 120000, Test reward: -422.72302978369555, Pid 2103\n",
      "\u001b[32m[06-24 19:18:21 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -678.3010998954585, Pid 2091\n",
      "\u001b[32m[06-24 19:27:42 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -939.2012403735891, Pid 2104\n",
      "\u001b[32m[06-24 19:29:36 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -801.117204512005, Pid 2099\n",
      "\u001b[32m[06-24 19:32:58 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -425.274197177968, Pid 2105\n",
      "\u001b[32m[06-24 19:35:07 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -758.7746289415647, Pid 2096\n",
      "\u001b[32m[06-24 19:35:15 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -602.3001855715631, Pid 2106\n",
      "\u001b[32m[06-24 19:35:33 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -136.50974897966313, Pid 2097\n",
      "\u001b[32m[06-24 19:35:38 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00017111111111111114  critic_lr: 0.008555555555555556  Pid 2097 ckpt model_dir/steps_130000_evaluate_reward_-136_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 19:35:46 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -956.4884366950198, Pid 2098\n",
      "\u001b[32m[06-24 19:35:57 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -910.1621056233096, Pid 2101\n",
      "\u001b[32m[06-24 19:36:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -872.2126228206167, Pid 2092\n",
      "\u001b[32m[06-24 19:37:10 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -666.7279454961126, Pid 2095\n",
      "\u001b[32m[06-24 19:38:32 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -413.8580621833338, Pid 2100\n",
      "\u001b[32m[06-24 19:39:02 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -636.5404060488122, Pid 2102\n",
      "\u001b[32m[06-24 19:39:46 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -25.798640171402404, Pid 2093\n",
      "\u001b[32m[06-24 19:39:50 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 8.555555555555557e-05  critic_lr: 0.008555555555555556  Pid 2093 ckpt model_dir/steps_130000_evaluate_reward_-25_ACTOR_LR_0.0001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 19:40:30 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 130000, Test reward: -361.7469229280249, Pid 2103\n",
      "\u001b[32m[06-24 19:42:09 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -597.7732324663257, Pid 2091\n",
      "\u001b[32m[06-24 19:42:12 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.88888888888889e-05  critic_lr: 0.0007888888888888889  Pid 2091 ckpt model_dir/steps_190000_evaluate_reward_-597_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 20:01:00 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -915.2745638911089, Pid 2104\n",
      "\u001b[32m[06-24 20:03:54 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -813.7819988104201, Pid 2099\n",
      "\u001b[32m[06-24 20:06:25 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -530.1607985418766, Pid 2091\n",
      "\u001b[32m[06-24 20:06:27 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.777777777777778e-05  critic_lr: 0.0007777777777777778  Pid 2091 ckpt model_dir/steps_200000_evaluate_reward_-530_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 20:06:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -581.7503992799435, Pid 2105\n",
      "\u001b[32m[06-24 20:08:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -421.74474319542253, Pid 2096\n",
      "\u001b[32m[06-24 20:08:43 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00016888888888888889  critic_lr: 0.004222222222222223  Pid 2096 ckpt model_dir/steps_140000_evaluate_reward_-421_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 20:08:54 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -653.1854457828563, Pid 2106\n",
      "\u001b[32m[06-24 20:09:07 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -937.4556757804467, Pid 2098\n",
      "\u001b[32m[06-24 20:09:12 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -135.71852003505967, Pid 2097\n",
      "\u001b[32m[06-24 20:09:15 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00016888888888888889  critic_lr: 0.008444444444444445  Pid 2097 ckpt model_dir/steps_140000_evaluate_reward_-135_ACTOR_LR_0.0002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 20:10:15 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -929.9897998392607, Pid 2101\n",
      "\u001b[32m[06-24 20:10:19 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -816.1194797125743, Pid 2092\n",
      "\u001b[32m[06-24 20:10:33 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -859.6467930449832, Pid 2095\n",
      "\u001b[32m[06-24 20:12:18 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -856.4710842981638, Pid 2102\n",
      "\u001b[32m[06-24 20:12:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -88.6964677557196, Pid 2100\n",
      "\u001b[32m[06-24 20:12:44 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004222222222222222  critic_lr: 0.008444444444444445  Pid 2100 ckpt model_dir/steps_140000_evaluate_reward_-88_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 20:13:44 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -77.01913342193858, Pid 2093\n",
      "\u001b[32m[06-24 20:13:49 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 140000, Test reward: -401.20407654693753, Pid 2103\n",
      "\u001b[32m[06-24 20:31:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 210000, Test reward: -452.0446451767078, Pid 2091\n",
      "\u001b[32m[06-24 20:31:39 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.666666666666667e-05  critic_lr: 0.0007666666666666666  Pid 2091 ckpt model_dir/steps_210000_evaluate_reward_-452_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 20:34:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -869.4150323115551, Pid 2104\n",
      "\u001b[32m[06-24 20:37:46 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -859.5024436945889, Pid 2099\n",
      "\u001b[32m[06-24 20:40:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -601.2208148638853, Pid 2105\n",
      "\u001b[32m[06-24 20:42:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -791.0641423872818, Pid 2106\n",
      "\u001b[32m[06-24 20:42:51 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -312.36975437260315, Pid 2097\n",
      "\u001b[32m[06-24 20:43:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -690.4447817512846, Pid 2096\n",
      "\u001b[32m[06-24 20:43:37 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -952.5580727298453, Pid 2098\n",
      "\u001b[32m[06-24 20:43:56 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -847.0845848514161, Pid 2092\n",
      "\u001b[32m[06-24 20:44:17 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -862.0906791886991, Pid 2095\n",
      "\u001b[32m[06-24 20:44:35 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -777.7325847938737, Pid 2101\n",
      "\u001b[32m[06-24 20:46:47 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -797.8207399480691, Pid 2102\n",
      "\u001b[32m[06-24 20:47:29 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -58.74369449738864, Pid 2100\n",
      "\u001b[32m[06-24 20:47:35 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004166666666666667  critic_lr: 0.008333333333333333  Pid 2100 ckpt model_dir/steps_150000_evaluate_reward_-58_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 20:48:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -353.7526145067372, Pid 2103\n",
      "\u001b[32m[06-24 20:48:58 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 150000, Test reward: -35.99158032959732, Pid 2093\n",
      "\u001b[32m[06-24 20:54:45 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 220000, Test reward: -390.4326686014964, Pid 2091\n",
      "\u001b[32m[06-24 20:54:48 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.555555555555556e-05  critic_lr: 0.0007555555555555555  Pid 2091 ckpt model_dir/steps_220000_evaluate_reward_-390_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 21:09:58 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -908.6053094650167, Pid 2104\n",
      "\u001b[32m[06-24 21:13:14 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -753.7535586929349, Pid 2099\n",
      "\u001b[32m[06-24 21:15:48 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -685.8107631181816, Pid 2105\n",
      "\u001b[32m[06-24 21:18:19 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -713.3981345933555, Pid 2106\n",
      "\u001b[32m[06-24 21:18:19 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 230000, Test reward: -325.3508323530837, Pid 2091\n",
      "\u001b[32m[06-24 21:18:23 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.444444444444444e-05  critic_lr: 0.0007444444444444445  Pid 2091 ckpt model_dir/steps_230000_evaluate_reward_-325_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 21:18:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -490.6331800075603, Pid 2097\n",
      "\u001b[32m[06-24 21:19:16 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -891.3296335337434, Pid 2098\n",
      "\u001b[32m[06-24 21:19:32 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -474.7200617026826, Pid 2096\n",
      "\u001b[32m[06-24 21:20:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -943.0358273782125, Pid 2095\n",
      "\u001b[32m[06-24 21:20:08 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -819.4532278617984, Pid 2092\n",
      "\u001b[32m[06-24 21:20:28 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -667.642295852175, Pid 2101\n",
      "\u001b[32m[06-24 21:20:32 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0008222222222222222  critic_lr: 0.0008222222222222222  Pid 2101 ckpt model_dir/steps_160000_evaluate_reward_-667_ACTOR_LR_0.001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 21:22:30 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -847.0069877395068, Pid 2102\n",
      "\u001b[32m[06-24 21:23:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -78.28904810726185, Pid 2100\n",
      "\u001b[32m[06-24 21:24:23 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -298.37242531461203, Pid 2103\n",
      "\u001b[32m[06-24 21:24:45 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 160000, Test reward: -124.55926653062014, Pid 2093\n",
      "\u001b[32m[06-24 21:41:43 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 240000, Test reward: -199.32448169918712, Pid 2091\n",
      "\u001b[32m[06-24 21:41:46 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.333333333333334e-05  critic_lr: 0.0007333333333333334  Pid 2091 ckpt model_dir/steps_240000_evaluate_reward_-199_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 21:44:40 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -894.7286621486313, Pid 2104\n",
      "\u001b[32m[06-24 21:48:21 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -624.0063402236816, Pid 2099\n",
      "\u001b[32m[06-24 21:50:51 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -182.51634030896625, Pid 2105\n",
      "\u001b[32m[06-24 21:50:55 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0016222222222222222  critic_lr: 0.004055555555555555  Pid 2105 ckpt model_dir/steps_170000_evaluate_reward_-182_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 21:52:50 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -220.97626605635065, Pid 2097\n",
      "\u001b[32m[06-24 21:53:14 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -499.5518576024025, Pid 2106\n",
      "\u001b[32m[06-24 21:53:55 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -976.1552779597445, Pid 2098\n",
      "\u001b[32m[06-24 21:54:08 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -906.9317980558599, Pid 2095\n",
      "\u001b[32m[06-24 21:54:12 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -596.2753415758071, Pid 2096\n",
      "\u001b[32m[06-24 21:54:26 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -936.7538745646777, Pid 2092\n",
      "\u001b[32m[06-24 21:54:46 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -899.8991769007592, Pid 2101\n",
      "\u001b[32m[06-24 21:57:34 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -633.2456944621738, Pid 2102\n",
      "\u001b[32m[06-24 21:58:13 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -53.12227930448266, Pid 2100\n",
      "\u001b[32m[06-24 21:58:19 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00040555555555555554  critic_lr: 0.00811111111111111  Pid 2100 ckpt model_dir/steps_170000_evaluate_reward_-53_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 21:59:11 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -26.159870413405553, Pid 2103\n",
      "\u001b[32m[06-24 21:59:15 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0008111111111111111  critic_lr: 0.00811111111111111  Pid 2103 ckpt model_dir/steps_170000_evaluate_reward_-26_ACTOR_LR_0.001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 21:59:40 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 170000, Test reward: -161.4996408311625, Pid 2093\n",
      "\u001b[32m[06-24 22:05:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 250000, Test reward: -59.3122072805849, Pid 2091\n",
      "\u001b[32m[06-24 22:05:09 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.222222222222222e-05  critic_lr: 0.0007222222222222222  Pid 2091 ckpt model_dir/steps_250000_evaluate_reward_-59_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 22:18:57 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -818.1716238410384, Pid 2104\n",
      "\u001b[32m[06-24 22:19:02 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0016  critic_lr: 0.0008  Pid 2104 ckpt model_dir/steps_180000_evaluate_reward_-818_ACTOR_LR_0.002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 22:23:22 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -595.0002901023056, Pid 2099\n",
      "\u001b[32m[06-24 22:25:38 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -137.30455631887685, Pid 2105\n",
      "\u001b[32m[06-24 22:25:42 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0016  critic_lr: 0.004  Pid 2105 ckpt model_dir/steps_180000_evaluate_reward_-137_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 22:27:29 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -251.8013800802239, Pid 2097\n",
      "\u001b[32m[06-24 22:28:13 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 260000, Test reward: -22.073456152122567, Pid 2091\n",
      "\u001b[32m[06-24 22:28:15 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.111111111111112e-05  critic_lr: 0.0007111111111111111  Pid 2091 ckpt model_dir/steps_260000_evaluate_reward_-22_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 22:28:17 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -363.22582289352914, Pid 2106\n",
      "\u001b[32m[06-24 22:28:43 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -995.4151238240868, Pid 2098\n",
      "\u001b[32m[06-24 22:28:49 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -715.4897962879737, Pid 2095\n",
      "\u001b[32m[06-24 22:29:03 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -823.6652942829736, Pid 2092\n",
      "\u001b[32m[06-24 22:29:10 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -803.3828941594033, Pid 2096\n",
      "\u001b[32m[06-24 22:29:33 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -971.1783788747568, Pid 2101\n",
      "\u001b[32m[06-24 22:31:59 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -844.6491381789328, Pid 2102\n",
      "\u001b[32m[06-24 22:33:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -20.508703007399266, Pid 2100\n",
      "\u001b[32m[06-24 22:33:09 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0004  critic_lr: 0.008  Pid 2100 ckpt model_dir/steps_180000_evaluate_reward_-20_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 22:33:33 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -98.52914749768938, Pid 2103\n",
      "\u001b[32m[06-24 22:33:57 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 180000, Test reward: -35.03161671547537, Pid 2093\n",
      "\u001b[32m[06-24 22:51:14 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 270000, Test reward: -20.83232294219092, Pid 2091\n",
      "\u001b[32m[06-24 22:51:19 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7e-05  critic_lr: 0.0007  Pid 2091 ckpt model_dir/steps_270000_evaluate_reward_-20_ACTOR_LR_0.0001_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 22:53:29 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -817.4173375470676, Pid 2104\n",
      "\u001b[32m[06-24 22:53:33 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0015777777777777778  critic_lr: 0.0007888888888888889  Pid 2104 ckpt model_dir/steps_190000_evaluate_reward_-817_ACTOR_LR_0.002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 22:57:26 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -354.68905226749825, Pid 2099\n",
      "\u001b[32m[06-24 22:57:30 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00039444444444444444  critic_lr: 0.003944444444444444  Pid 2099 ckpt model_dir/steps_190000_evaluate_reward_-354_ACTOR_LR_0.0005_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 22:59:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -423.8024414632099, Pid 2105\n",
      "\u001b[32m[06-24 23:01:36 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -257.3753202967951, Pid 2097\n",
      "\u001b[32m[06-24 23:02:47 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -438.7657327870878, Pid 2106\n",
      "\u001b[32m[06-24 23:03:11 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -981.0506681661315, Pid 2098\n",
      "\u001b[32m[06-24 23:03:14 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -894.313472583967, Pid 2092\n",
      "\u001b[32m[06-24 23:03:20 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -650.6331829982089, Pid 2095\n",
      "\u001b[32m[06-24 23:03:56 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -326.36856373680985, Pid 2096\n",
      "\u001b[32m[06-24 23:04:00 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0001577777777777778  critic_lr: 0.003944444444444444  Pid 2096 ckpt model_dir/steps_190000_evaluate_reward_-326_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 23:04:04 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -921.8974855502076, Pid 2101\n",
      "\u001b[32m[06-24 23:06:16 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -862.6455878322047, Pid 2102\n",
      "\u001b[32m[06-24 23:08:41 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -21.641576955743613, Pid 2100\n",
      "\u001b[32m[06-24 23:08:58 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -138.5594404353589, Pid 2103\n",
      "\u001b[32m[06-24 23:09:10 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 190000, Test reward: -21.75897434724279, Pid 2093\n",
      "\u001b[32m[06-24 23:09:13 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.88888888888889e-05  critic_lr: 0.007888888888888888  Pid 2093 ckpt model_dir/steps_190000_evaluate_reward_-21_ACTOR_LR_0.0001_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 23:14:03 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 280000, Test reward: -21.181337010550315, Pid 2091\n",
      "\u001b[32m[06-24 23:28:29 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -784.718871531031, Pid 2104\n",
      "\u001b[32m[06-24 23:28:32 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0015555555555555557  critic_lr: 0.0007777777777777778  Pid 2104 ckpt model_dir/steps_200000_evaluate_reward_-784_ACTOR_LR_0.002_CRITIC_LR_0.001_model_tag_2\n",
      "\u001b[32m[06-24 23:31:54 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -138.93207458347277, Pid 2099\n",
      "\u001b[32m[06-24 23:31:58 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0003888888888888889  critic_lr: 0.003888888888888889  Pid 2099 ckpt model_dir/steps_200000_evaluate_reward_-138_ACTOR_LR_0.0005_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 23:33:58 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -46.44926322954759, Pid 2105\n",
      "\u001b[32m[06-24 23:34:03 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0015555555555555557  critic_lr: 0.003888888888888889  Pid 2105 ckpt model_dir/steps_200000_evaluate_reward_-46_ACTOR_LR_0.002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 23:36:13 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -164.2427618516862, Pid 2097\n",
      "\u001b[32m[06-24 23:36:49 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -125.61568507419986, Pid 2106\n",
      "\u001b[32m[06-24 23:36:53 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.0015555555555555557  critic_lr: 0.007777777777777778  Pid 2106 ckpt model_dir/steps_200000_evaluate_reward_-125_ACTOR_LR_0.002_CRITIC_LR_0.01_model_tag_2\n",
      "\u001b[32m[06-24 23:37:25 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -998.7418972435411, Pid 2098\n",
      "\u001b[32m[06-24 23:37:31 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -578.2731808621731, Pid 2092\n",
      "\u001b[32m[06-24 23:37:54 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -391.05277844769597, Pid 2095\n",
      "\u001b[32m[06-24 23:38:14 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 290000, Test reward: -21.038376223579, Pid 2091\n",
      "\u001b[32m[06-24 23:38:27 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -963.3182341659365, Pid 2101\n",
      "\u001b[32m[06-24 23:38:38 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -262.66068369596155, Pid 2096\n",
      "\u001b[32m[06-24 23:38:42 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 0.00015555555555555556  critic_lr: 0.003888888888888889  Pid 2096 ckpt model_dir/steps_200000_evaluate_reward_-262_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2\n",
      "\u001b[32m[06-24 23:40:51 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -479.6292420785323, Pid 2102\n",
      "\u001b[32m[06-24 23:43:06 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -21.69500408208086, Pid 2100\n",
      "\u001b[32m[06-24 23:43:39 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -74.13259997436134, Pid 2103\n",
      "\u001b[32m[06-24 23:43:43 MainThread @<ipython-input-9-8259d0fb9623>:52]\u001b[0m Steps 200000, Test reward: -21.07723632020937, Pid 2093\n",
      "\u001b[32m[06-24 23:43:47 MainThread @<ipython-input-9-8259d0fb9623>:60]\u001b[0m Current actor_lr: 7.777777777777778e-05  critic_lr: 0.007777777777777778  Pid 2093 ckpt model_dir/steps_200000_evaluate_reward_-21_ACTOR_LR_0.0001_CRITIC_LR_0.01_model_tag_2\n"
     ]
    }
   ],
   "source": [
    "DEBUG = False\n",
    "%matplotlib inline\n",
    "parallel(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall paddlepaddle -y\n",
    "# !pip install paddlepaddle=1.8.2\n",
    "# !pip install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/PaddlePaddle/PARL/blob/develop/examples/tutorials/homework/lesson5/ddpg_quadrotor/quadrotor_agent.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "\n",
    "\n",
    "class BipedalWalkerAgent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim=4):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(BipedalWalkerAgent, self).__init__(algorithm)\n",
    "\n",
    "        # 注意，在最开始的时候，先完全同步target_model和model的参数\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n",
    "                                                 terminal)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        critic_cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n",
    "        self.alg.sync_target()\n",
    "\n",
    "        return critic_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/PaddlePaddle/PARL/blob/develop/examples/tutorials/homework/lesson5/ddpg_quadrotor/quadrotor_model.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "import parl\n",
    "from parl import layers\n",
    "\n",
    "\n",
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim, model_tag):\n",
    "        self.model_tag = model_tag\n",
    "        if self.model_tag == 1:\n",
    "            # simple model\n",
    "            hid1_size = 100\n",
    "            hid2_size = 100\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=act_dim, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "        else:\n",
    "            hid1_size = 128\n",
    "            hid2_size = 256\n",
    "            hid3_size = 128\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=hid3_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc4 = layers.fc(size=act_dim, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        if self.model_tag == 1:\n",
    "            hid = self.fc1(obs)\n",
    "            hid = self.fc2(hid)\n",
    "            logits = self.fc3(hid)\n",
    "        else:\n",
    "            hid1 = self.fc1(obs)\n",
    "            hid2 = self.fc2(hid1)\n",
    "            hid3 = self.fc3(hid2)\n",
    "            logits = self.fc4(hid3)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CriticModel(parl.Model):\n",
    "    def __init__(self, model_tag):\n",
    "        self.model_tag = model_tag\n",
    "        if self.model_tag == 1:\n",
    "            hid_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=1, act=None)\n",
    "        else:\n",
    "            hid1_size = 100\n",
    "            hid2_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=1, act=None)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        # 输入 state, action, 输出对应的Q(s,a)\n",
    "        if self.model_tag == 1:\n",
    "            concat = layers.concat([obs, act], axis=1)\n",
    "            hid = self.fc1(concat)\n",
    "            Q = self.fc2(hid)\n",
    "            Q = layers.squeeze(Q, axes=[1])\n",
    "        else:\n",
    "            hid1 = self.fc1(obs)\n",
    "            concat = layers.concat([hid1, act], axis=1)\n",
    "            hid2 = self.fc2(concat)\n",
    "            Q = self.fc3(hid2)\n",
    "            Q = layers.squeeze(Q, axes=[1])\n",
    "        return Q\n",
    "\n",
    "\n",
    "class BipedalWalkerModel(parl.Model):\n",
    "    def __init__(self, act_dim, model_tag):\n",
    "        self.model_tag = model_tag\n",
    "        self.actor_model = ActorModel(act_dim, self.model_tag)\n",
    "        self.critic_model = CriticModel(self.model_tag)\n",
    "\n",
    "    def policy(self, obs):\n",
    "        return self.actor_model.policy(obs)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        return self.critic_model.value(obs, act)\n",
    "\n",
    "    def get_actor_params(self):\n",
    "        return self.actor_model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/PaddlePaddle/PARL/blob/develop/examples/tutorials/homework/lesson5/ddpg_quadrotor/train.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "from parl.utils import logger\n",
    "from parl.utils import action_mapping # 将神经网络输出映射到对应的 实际动作取值范围内\n",
    "from parl.utils import ReplayMemory # 经验回放\n",
    "from parl.algorithms import DDPG\n",
    "from parl.utils.scheduler import LinearDecayScheduler, PiecewiseScheduler\n",
    "\n",
    "# from model import BipedalWalkerModel\n",
    "# from agent import BipedalWalkerAgent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def run_episode(env, agent, rpm):\n",
    "    obs = env.reset()\n",
    "    # print(obs)\n",
    "    total_reward, steps = 0, 0\n",
    "    while True:\n",
    "        steps += 1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "        action = np.squeeze(action)\n",
    "\n",
    "        # 给输出动作增加探索扰动，输出限制在 [-1.0, 1.0] 范围内\n",
    "        action = np.clip(np.random.normal(action, 1.0), -1.0, 1.0)\n",
    "        # 动作映射到对应的 实际动作取值范围 内, action_mapping是从parl.utils那里import进来的函数\n",
    "        action = action_mapping(action, env.action_space.low[0],\n",
    "                                env.action_space.high[0])\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        rpm.append(obs, action, REWARD_SCALE * reward, next_obs, done)\n",
    "\n",
    "        if rpm.size() > MEMORY_WARMUP_SIZE:\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, \\\n",
    "            batch_terminal = rpm.sample_batch(BATCH_SIZE)\n",
    "            critic_cost = agent.learn(batch_obs, batch_action, batch_reward,\n",
    "                                      batch_next_obs, batch_terminal)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, steps\n",
    "\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        total_reward, steps = 0, 0\n",
    "        while True:\n",
    "            batch_obs = np.expand_dims(obs, axis=0)\n",
    "            action = agent.predict(batch_obs.astype('float32'))\n",
    "            action = np.squeeze(action)\n",
    "            action = np.clip(action, -1.0, 1.0)  # the action should be in range [-1.0, 1.0]\n",
    "            action = action_mapping(action, env.action_space.low[0],\n",
    "                                    env.action_space.high[0])\n",
    "\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(total_reward)\n",
    "    return np.mean(eval_reward)\n",
    "\n",
    "\n",
    "def draw_results(train_score_list, test_score_list, title='', path='./fig_dir/'):\n",
    "    # 画出训练过程reward历史曲线\n",
    "    plt.figure(figsize=[14,6])\n",
    "    plt.subplot(121)\n",
    "    plt.plot(train_score_list, color='green', label='train')\n",
    "    plt.title('Train History {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(test_score_list, color='red', label='test')\n",
    "    plt.title('Test History {}'.format(title))\n",
    "    plt.legend()\n",
    "    if path != '' and not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    plt.savefig(path + title + '.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main(ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=1, load_model=False, go_steps=1, f_best=''):\n",
    "    # 创建环境\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    env.reset()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # 根据parl框架构建agent\n",
    "    model = BipedalWalkerModel(act_dim, model_tag=model_tag)\n",
    "    algorithm = DDPG(model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "    agent = BipedalWalkerAgent(algorithm, obs_dim, act_dim)\n",
    "\n",
    "    # parl库也为DDPG算法内置了ReplayMemory，可直接从 parl.utils 引入使用\n",
    "    rpm = ReplayMemory(int(MEMORY_SIZE), obs_dim, act_dim)\n",
    "\n",
    "    # 启动训练\n",
    "    logger.info(\n",
    "        'Params: ACTOR_LR={}, CRITIC_LR={}, model_tag={}, Pid {}'.format(ACTOR_LR, CRITIC_LR, model_tag, os.getpid()))\n",
    "    test_flag = 0\n",
    "    total_steps = 0\n",
    "    early_stop = 0\n",
    "    last_reward = -1e9\n",
    "    best_reward = -1e9\n",
    "    # 与下文结合，使得学习率在训练到90%时保持初始学习率的1%水平继续训练\n",
    "    actor_lr_scheduler = [(0, ACTOR_LR), (int(TRAIN_TOTAL_STEPS/5), ACTOR_LR/5), (int(TRAIN_TOTAL_STEPS/2), ACTOR_LR/10)]\n",
    "    critic_lr_scheduler = [(0, CRITIC_LR), (int(TRAIN_TOTAL_STEPS/5), CRITIC_LR/5), (int(TRAIN_TOTAL_STEPS/2), CRITIC_LR/10)]\n",
    "    actor_lr_scheduler = PiecewiseScheduler(actor_lr_scheduler)\n",
    "    critic_lr_scheduler = PiecewiseScheduler(critic_lr_scheduler)\n",
    "    train_score_list = []\n",
    "    test_score_list = []\n",
    "\n",
    "    # load best results and continue training\n",
    "    if load_model == True and os.path.exists(f_best + '.ckpt'):\n",
    "        agent.restore(f_best + '.ckpt')\n",
    "        rpm.load('rpm_{}'.format(model_tag))\n",
    "        actor_lr_scheduler.step(step_num=go_steps)\n",
    "        critic_lr_scheduler.step(step_num=go_steps)\n",
    "        logger.info('load model success. Pid {}'.format(os.getpid()))\n",
    "\n",
    "    while total_steps < TRAIN_TOTAL_STEPS:\n",
    "        train_reward, steps = run_episode(env, agent, rpm)\n",
    "        total_steps += steps\n",
    "        # logger.info('Steps: {} Reward: {} Pid: {}'.format(total_steps, train_reward, os.getpid())) # 打印训练reward\n",
    "        train_score_list.append(train_reward)\n",
    "\n",
    "        # 可以在这里修改学习率, 可以用 parl.utils.scheduler 中的 LinearDecayScheduler 进行修改，也可以自行修改\n",
    "        agent.alg.actor_lr = max(actor_lr_scheduler.step(step_num=steps), ACTOR_LR / 100)\n",
    "        agent.alg.critic_lr = max(critic_lr_scheduler.step(step_num=steps), CRITIC_LR / 100)\n",
    "\n",
    "        if total_steps // TEST_EVERY_STEPS >= test_flag:  # 每隔一定step数，评估一次模型\n",
    "            while total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "                test_flag += 1\n",
    "\n",
    "            evaluate_reward = evaluate(env, agent)\n",
    "            test_score_list.append(evaluate_reward)\n",
    "            logger.info(\n",
    "                'Steps {}, Test reward: {}, Pid {}'.format(total_steps, evaluate_reward, os.getpid()))  # 打印评估的reward\n",
    "\n",
    "            with open('train_{}.pickle'.format(model_tag), 'wb') as f:\n",
    "                pickle.dump([train_score_list, test_score_list], f)\n",
    "                print('history saved')\n",
    "                    \n",
    "            # 每评估一次，优于最优模型就保存一次模型和记忆回放，以训练的step数命名，DEBUG时则一直保存模型和图片\n",
    "            if evaluate_reward > best_reward or DEBUG:  # velocity control task reward will always be negative\n",
    "                ckpt = 'model_dir/steps_{}_evaluate_reward_{}_ACTOR_LR_{}_CRITIC_LR_{}_model_tag_{}'.format(total_steps,\n",
    "                                                                                                            int(\n",
    "                                                                                                                evaluate_reward),\n",
    "                                                                                                            ACTOR_LR,\n",
    "                                                                                                            CRITIC_LR,\n",
    "                                                                                                            model_tag)\n",
    "                agent.save(ckpt + '.ckpt')\n",
    "                rpm.save('rpm_{}'.format(model_tag))\n",
    "                logger.info('Current actor_lr: {}  critic_lr: {}  Pid {} ckpt {}'.format(agent.alg.actor_lr,\n",
    "                                                                                         agent.alg.critic_lr,\n",
    "                                                                                         os.getpid(), ckpt))\n",
    "\n",
    "                # 每次保存模型时画出当前reward趋势图\n",
    "                draw_results(train_score_list, test_score_list, '_'.join([str(ACTOR_LR), str(CRITIC_LR), str(model_tag), str(total_steps)]))\n",
    "\n",
    "                # update best reward\n",
    "                best_reward = evaluate_reward\n",
    "\n",
    "            # early_stop, 超过20%训练进度且连续5次测评reward下降则提前终止\n",
    "            if evaluate_reward > last_reward:\n",
    "                early_stop = 0\n",
    "            else:\n",
    "                early_stop += 1\n",
    "            last_reward = evaluate_reward\n",
    "            if total_steps > TRAIN_TOTAL_STEPS / 5 and early_stop >= 5:\n",
    "                logger.info(\n",
    "                    'No good results, stop training. Params: ACTOR_LR={}, CRITIC_LR={}, model_tag={}, Pid {}'.format(\n",
    "                        ACTOR_LR, CRITIC_LR, model_tag, os.getpid()))\n",
    "                break\n",
    "    # 训练结束，画出reward趋势图，并保存最终模型\n",
    "    draw_results(train_score_list, test_score_list, '_'.join([str(ACTOR_LR), str(CRITIC_LR), str(model_tag), str(total_steps)]))\n",
    "    ckpt = 'model_dir/steps_{}_evaluate_reward_{}_ACTOR_LR_{}_CRITIC_LR_{}_model_tag_{}'.format(total_steps,\n",
    "                                                                                                int(evaluate_reward),\n",
    "                                                                                                ACTOR_LR, CRITIC_LR,\n",
    "                                                                                                model_tag)\n",
    "    agent.save(ckpt + '.ckpt')\n",
    "    rpm.save('rpm_{}'.format(model_tag))\n",
    "    with open('train_{}.pickle'.format(model_tag), 'wb') as f:\n",
    "        pickle.dump([train_score_list, test_score_list], f)\n",
    "        print('history saved')\n",
    "    logger.info('Current actor_lr: {}  critic_lr: {}  Pid {} ckpt {}'.format(agent.alg.actor_lr, agent.alg.critic_lr,\n",
    "                                                                             os.getpid(), ckpt))\n",
    "\n",
    "\n",
    "def parallel(num_cores=2, num_gpus=0):\n",
    "    assert isinstance(num_cores, int)\n",
    "    assert num_cores > 0\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    # 多进程不能使用GPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "    print('Parent process %s.' % os.getpid())\n",
    "\n",
    "    p = Pool(num_cores)\n",
    "    used_gpus = 0\n",
    "    for ACTOR_LR in [0.0001, 0.0002, 0.0005, 0.001, 0.002]:  # 0.0002\n",
    "        for CRITIC_LR in [0.001, 0.005, 0.01]:  # 0.001\n",
    "            for model_tag in [2]:\n",
    "                p.apply_async(main, args=(ACTOR_LR, CRITIC_LR, model_tag))\n",
    "\n",
    "    print('Waiting for all subprocesses done...')\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print('All subprocesses done.')\n",
    "\n",
    "\n",
    "def one(ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=2, load_model=False, go_steps=1, f_best='', gpu=''):\n",
    "    # 默认不使用GPU，在我的服务器出现以下错误：\n",
    "    # ExternalError:  Cublas error, CUBLAS_STATUS_NOT_INITIALIZED  at (/paddle/paddle/fluid/platform/cuda_helper.h:32)\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "    main(ACTOR_LR, CRITIC_LR, model_tag, load_model, go_steps, f_best)\n",
    "\n",
    "\n",
    "def find_best():\n",
    "    best = -1000000\n",
    "    best_f = ''\n",
    "    for _, _, files in os.walk('./model_dir'):\n",
    "        for f in files:\n",
    "            if f[-4:] == 'ckpt':\n",
    "                reward = int(f[f.find('reward') + 7:f.find('ACTOR') - 1])\n",
    "                if reward >= best:\n",
    "                    best = reward\n",
    "                    best_f = f\n",
    "    return 'model_dir/' + best_f[:-5]\n",
    "\n",
    "\n",
    "def fine_tune(ACTOR_LR=0.0002, CRITIC_LR=0.005, episodes=10, model_tag=2, load_model=True, go_steps=1):\n",
    "    for i in range(episodes):\n",
    "        f_best = find_best()\n",
    "        logger.info('Current best: {}, finetune it...'.format(f_best))\n",
    "        # todo: extract go_steps from f_best\n",
    "        one(ACTOR_LR=ACTOR_LR, CRITIC_LR=CRITIC_LR, model_tag=model_tag, load_model=load_model, go_steps=go_steps, f_best=f_best)\n",
    "\n",
    "\n",
    "def test(total_steps, evaluate_reward, ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=2, render=False):\n",
    "    # 请设置ckpt为你训练中效果最好的一次评估保存的模型文件名称\n",
    "    ckpt = 'model_dir/steps_{}_evaluate_reward_{}_ACTOR_LR_{}_CRITIC_LR_{}_model_tag_{}.ckpt'.format(total_steps, int(evaluate_reward), ACTOR_LR, CRITIC_LR, model_tag)\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    env.reset()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    model = BipedalWalkerModel(act_dim, model_tag=model_tag)\n",
    "    algorithm = DDPG(model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "    agent = BipedalWalkerAgent(algorithm, obs_dim, act_dim)\n",
    "    # 加载模型\n",
    "    if os.path.exists(ckpt):\n",
    "        agent.restore(ckpt)\n",
    "        logger.info('Test Model file {}'.format(ckpt))\n",
    "    else:\n",
    "        logger.info('No Model file {}'.format(ckpt))\n",
    "        return -1\n",
    "    evaluate_reward = evaluate(env, agent, render=render)\n",
    "    logger.info('Evaluate reward: {}'.format(evaluate_reward)) # 打印评估的reward\n",
    "\n",
    "\n",
    "def test_best(render=False):\n",
    "    best = -1000000\n",
    "    best_f = ''\n",
    "    for _, _, files in os.walk('./model_dir'):\n",
    "        for f in files:\n",
    "            if f[-4:] == 'ckpt':\n",
    "                reward = int(f[f.find('reward')+7:f.find('ACTOR')-1])\n",
    "                if reward >= best:\n",
    "                    best = reward\n",
    "                    best_f = f\n",
    "    res = best_f.split('_')\n",
    "    test(res[1], res[4], float(res[7]), float(res[10]), int(res[-1].split('.')[0]), render=render)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTOR_LR = 0.0001  # Actor网络更新的 learning rate\n",
    "CRITIC_LR = 0.001 # Critic网络更新的 learning rate\n",
    "\n",
    "GAMMA = 0.99  # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "TAU = 0.001  # target_model 跟 model 同步参数 的 软更新参数\n",
    "MEMORY_SIZE = 1e6  # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 1e3  # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn\n",
    "REWARD_SCALE = 0.01  # reward 的缩放因子\n",
    "BATCH_SIZE = 512  # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "TRAIN_TOTAL_STEPS = 1e6  # 总训练步数\n",
    "TEST_EVERY_STEPS = 1e3  # 每个N步评估一下算法效果，每次评估5个episode求平均reward\n",
    "\n",
    "DEBUG = False\n",
    "# parallel(15)  # train from beginning\n",
    "test_best(render=True)  # test current best model\n",
    "# fine_tune(ACTOR_LR=0.0001, CRITIC_LR=0.001, episodes=1e6, go_steps=1)  # finetune existing model \n",
    "# one(ACTOR_LR=0.0001, CRITIC_LR=0.001, model_tag=2, load_model=False, go_steps=1, f_best='', gpu='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
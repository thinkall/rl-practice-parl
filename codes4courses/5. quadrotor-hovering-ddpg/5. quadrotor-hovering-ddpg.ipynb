{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大作业：使用DDPG解决四轴飞行器悬浮任务\n",
    "\n",
    "# Step1 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling parl-1.3.1:\n",
      "  Successfully uninstalled parl-1.3.1\n",
      "\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping scikit-learn as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Skipping paddlepaddle-gpu as it is not installed.\u001b[0m\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: paddlepaddle==1.6.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (1.6.3)\n",
      "Requirement already satisfied: graphviz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.13)\n",
      "Requirement already satisfied: rarfile in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (7.1.2)\n",
      "Requirement already satisfied: prettytable in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (0.7.2)\n",
      "Requirement already satisfied: scipy; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.3.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.22.0)\n",
      "Requirement already satisfied: funcsigs in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.0.2)\n",
      "Requirement already satisfied: matplotlib; python_version >= \"3.6\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (2.2.3)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.1.1.26)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.12; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.16.4)\n",
      "Requirement already satisfied: nltk; python_version >= \"3.5\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (5.1.2)\n",
      "Requirement already satisfied: objgraph in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.4.1)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle==1.6.3) (3.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle==1.6.3) (1.25.6)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib; python_version >= \"3.6\"->paddlepaddle==1.6.3) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from protobuf>=3.1.0->paddlepaddle==1.6.3) (41.4.0)\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting parl==1.3.1\n",
      "\u001b[?25l  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/62/79/590af38a920792c71afb73fad7583967928b4d0ba9fca76250d935c7fda8/parl-1.3.1-py2.py3-none-any.whl (521kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle==1.2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.2.1)\n",
      "Requirement already satisfied: psutil>=5.6.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (5.7.0)\n",
      "Requirement already satisfied: pyzmq==18.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (18.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (7.0)\n",
      "Requirement already satisfied: flask-cors in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (3.0.8)\n",
      "Requirement already satisfied: tensorboardX==1.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.8)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: flask>=1.0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.1.1)\n",
      "Requirement already satisfied: visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (2.0.0b7)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: tb-nightly==1.15.0a20190801 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (1.15.0a20190801)\n",
      "Requirement already satisfied: pyarrow==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from parl==1.3.1) (0.13.0)\n",
      "Requirement already satisfied: Six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask-cors->parl==1.3.1) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboardX==1.8->parl==1.3.1) (3.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboardX==1.8->parl==1.3.1) (1.16.4)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (0.16.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (2.10.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.0.4->parl==1.3.1) (1.1.0)\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (3.7.9)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (4.1.1.26)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (7.1.2)\n",
      "Requirement already satisfied: hdfs in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.5.8)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.22.0)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.21.0)\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (3.1.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (41.4.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (0.33.6)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-nightly==1.15.0a20190801->parl==1.3.1) (1.26.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.0.4->parl==1.3.1) (1.1.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.3)\n",
      "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.5.0)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.1.1)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.6.1)\n",
      "Requirement already satisfied: docopt in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from hdfs->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.6.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.25.6)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.3.0)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (16.7.9)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.0.1)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.4.10)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (5.1.2)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.23)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (1.3.4)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2019.3)\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (2.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl>=2.0.0b; python_version >= \"3\" and platform_system == \"Linux\"->parl==1.3.1) (7.2.0)\n",
      "Installing collected packages: parl\n",
      "Successfully installed parl-1.3.1\n",
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: rlschool==0.3.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.3.1)\n",
      "Requirement already satisfied: numpy>=1.16.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.12.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (1.3.0)\n",
      "Requirement already satisfied: colour>=0.1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (0.1.5)\n",
      "Requirement already satisfied: pyglet==1.5.0; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (1.5.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (2.4)\n",
      "Requirement already satisfied: Pillow>=6.2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (7.1.2)\n",
      "Requirement already satisfied: configparser>=3.7.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (5.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (1.12.0)\n",
      "Requirement already satisfied: trimesh>=3.2.39 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rlschool==0.3.1) (3.7.0)\n",
      "Requirement already satisfied: future in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pyglet==1.5.0; python_version >= \"3\"->rlschool==0.3.1) (0.18.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from networkx>=2.2->rlschool==0.3.1) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trimesh>=3.2.39->rlschool==0.3.1) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y parl  # 说明：AIStudio预装的parl版本太老，容易跟其他库产生兼容性冲突，建议先卸载\n",
    "!pip uninstall -y pandas scikit-learn # 提示：在AIStudio中卸载这两个库再import parl可避免warning提示，不卸载也不影响parl的使用\n",
    "!pip uninstall -y paddlepaddle-gpu\n",
    "\n",
    "!pip install paddlepaddle==1.6.3 \n",
    "!pip install parl==1.3.1\n",
    "!pip install rlschool==0.3.1\n",
    "\n",
    "# 说明：安装日志中出现两条红色的关于 paddlehub 和 visualdl 的 ERROR 与parl无关，可以忽略，不影响使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlepaddle         1.6.3          \n",
      "parl                 1.3.1          \n",
      "rlschool             0.3.1          \n"
     ]
    }
   ],
   "source": [
    "# 检查依赖包版本是否正确\n",
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl\n",
    "!pip list | grep rlschool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import parl\n",
    "from parl import layers\n",
    "from paddle import fluid\n",
    "from parl.utils import logger\n",
    "from parl.utils import action_mapping # 将神经网络输出映射到对应的 实际动作取值范围 内\n",
    "from parl.utils import ReplayMemory # 经验回放\n",
    "\n",
    "from rlschool import make_env  # 使用 RLSchool 创建飞行器环境\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 1. 请设定 learning rate，尝试增减查看效果\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "ACTOR_LR = 0.0002   # Actor网络更新的 learning rate\n",
    "CRITIC_LR = 0.005   # Critic网络更新的 learning rate\n",
    "DEBUG = False\n",
    "\n",
    "GAMMA = 0.99        # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "TAU = 0.001         # target_model 跟 model 同步参数 的 软更新参数\n",
    "MEMORY_SIZE = 1e6   # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 1e4      # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn\n",
    "REWARD_SCALE = 0.01       # reward 的缩放因子\n",
    "BATCH_SIZE = 256          # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "TRAIN_TOTAL_STEPS = 1e6   # 总训练步数\n",
    "TEST_EVERY_STEPS = 1e4    # 每个N步评估一下算法效果，每次评估5个episode求平均reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 搭建Model、Algorithm、Agent架构\n",
    "* `Agent`把产生的数据传给`algorithm`，`algorithm`根据`model`的模型结构计算出`Loss`，使用`SGD`或者其他优化器不断的优化，`PARL`这种架构可以很方便的应用在各类深度强化学习问题中。\n",
    "\n",
    "## （1）Model\n",
    "* 分别搭建`Actor`、`Critic`的`Model`结构，构建`QuadrotorModel`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ActorModel(parl.Model):\n",
    "    def __init__(self, act_dim, model_tag):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 2. 请配置model结构\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        self.model_tag = model_tag\n",
    "        if self.model_tag == 1:\n",
    "            # simple model\n",
    "            hid_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=act_dim, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "        else:\n",
    "            hid1_size = 100\n",
    "            hid2_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=act_dim, act='tanh', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "\n",
    "    def policy(self, obs):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 3. 请组装policy网络\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        if self.model_tag == 1:\n",
    "            hid = self.fc1(obs)\n",
    "            logits = self.fc2(hid)\n",
    "        else:                \n",
    "            hid1 = self.fc1(obs)\n",
    "            hid2 = self.fc2(hid1)\n",
    "            logits = self.fc3(hid2) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class CriticModel(parl.Model):\n",
    "    def __init__(self, model_tag):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 4. 请配置model结构\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        self.model_tag = model_tag\n",
    "        if self.model_tag == 1:\n",
    "            hid_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=1, act=None)\n",
    "        else:\n",
    "            hid1_size = 100\n",
    "            hid2_size = 100\n",
    "\n",
    "            self.fc1 = layers.fc(size=hid1_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc2 = layers.fc(size=hid2_size, act='relu', param_attr=fluid.initializer.Normal(loc=0.0, scale=0.1))\n",
    "            self.fc3 = layers.fc(size=1, act=None)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        # 输入 state, action, 输出对应的Q(s,a)\n",
    "\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 5. 请组装Q网络\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        if self.model_tag == 1:\n",
    "            concat = layers.concat([obs, act], axis=1)\n",
    "            hid = self.fc1(concat)\n",
    "            Q = self.fc2(hid)\n",
    "            Q = layers.squeeze(Q, axes=[1])\n",
    "        else:\n",
    "            hid1 = self.fc1(obs)\n",
    "            concat = layers.concat([hid1, act], axis=1)\n",
    "            hid2 = self.fc2(concat)\n",
    "            Q = self.fc3(hid2)\n",
    "            Q = layers.squeeze(Q, axes=[1])\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class QuadrotorModel(parl.Model):\n",
    "    def __init__(self, act_dim, model_tag):\n",
    "        self.model_tag = model_tag\n",
    "        self.actor_model = ActorModel(act_dim, self.model_tag)\n",
    "        self.critic_model = CriticModel(self.model_tag)\n",
    "\n",
    "    def policy(self, obs):\n",
    "        return self.actor_model.policy(obs)\n",
    "\n",
    "    def value(self, obs, act):\n",
    "        return self.critic_model.value(obs, act)\n",
    "\n",
    "    def get_actor_params(self):\n",
    "        return self.actor_model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （2）Algorithm\n",
    "* 可以采用下面的方式从`parl`库中快速引入`DDPG`算法，无需自己重新写算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from parl.algorithms import DDPG\n",
    "\n",
    "import parl\n",
    "from parl import layers\n",
    "from copy import deepcopy\n",
    "from paddle import fluid\n",
    "\n",
    "\n",
    "class DDPG(parl.Algorithm):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 gamma=None,\n",
    "                 tau=None,\n",
    "                 actor_lr=None,\n",
    "                 critic_lr=None):\n",
    "        \"\"\"  DDPG algorithm\n",
    "        \n",
    "        Args:\n",
    "            model (parl.Model): actor and critic 的前向网络.\n",
    "                                model 必须实现 get_actor_params() 方法.\n",
    "            gamma (float): reward的衰减因子.\n",
    "            tau (float): self.target_model 跟 self.model 同步参数 的 软更新参数\n",
    "            actor_lr (float): actor 的学习率\n",
    "            critic_lr (float): critic 的学习率\n",
    "        \"\"\"\n",
    "        assert isinstance(gamma, float)\n",
    "        assert isinstance(tau, float)\n",
    "        assert isinstance(actor_lr, float)\n",
    "        assert isinstance(critic_lr, float)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "\n",
    "        self.model = model\n",
    "        self.target_model = deepcopy(model)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        \"\"\" 使用 self.model 的 actor model 来预测动作\n",
    "        \"\"\"\n",
    "        return self.model.policy(obs)\n",
    "\n",
    "    def learn(self, obs, action, reward, next_obs, terminal):\n",
    "        \"\"\" 用DDPG算法更新 actor 和 critic\n",
    "        \"\"\"\n",
    "        actor_cost = self._actor_learn(obs)\n",
    "        critic_cost = self._critic_learn(obs, action, reward, next_obs,\n",
    "                                         terminal)\n",
    "        return actor_cost, critic_cost\n",
    "\n",
    "    def _actor_learn(self, obs):\n",
    "        action = self.model.policy(obs)\n",
    "        Q = self.model.value(obs, action)\n",
    "        cost = layers.reduce_mean(-1.0 * Q)\n",
    "        optimizer = fluid.optimizer.AdamOptimizer(self.actor_lr)\n",
    "        optimizer.minimize(cost, parameter_list=self.model.get_actor_params())\n",
    "        if DEBUG:\n",
    "            print(self.actor_lr)\n",
    "        return cost\n",
    "\n",
    "    def _critic_learn(self, obs, action, reward, next_obs, terminal):\n",
    "        next_action = self.target_model.policy(next_obs)\n",
    "        next_Q = self.target_model.value(next_obs, next_action)\n",
    "\n",
    "        terminal = layers.cast(terminal, dtype='float32')\n",
    "        target_Q = reward + (1.0 - terminal) * self.gamma * next_Q\n",
    "        target_Q.stop_gradient = True\n",
    "\n",
    "        Q = self.model.value(obs, action)\n",
    "        cost = layers.square_error_cost(Q, target_Q)\n",
    "        cost = layers.reduce_mean(cost)\n",
    "        optimizer = fluid.optimizer.AdamOptimizer(self.critic_lr)\n",
    "        if DEBUG:\n",
    "            print(self.critic_lr)\n",
    "        optimizer.minimize(cost)\n",
    "        return cost\n",
    "\n",
    "    def sync_target(self, decay=None, share_vars_parallel_executor=None):\n",
    "        \"\"\" self.target_model从self.model复制参数过来，若decay不为None,则是软更新\n",
    "        \"\"\"\n",
    "        if decay is None:\n",
    "            decay = 1.0 - self.tau\n",
    "        self.model.sync_weights_to(\n",
    "            self.target_model,\n",
    "            decay=decay,\n",
    "            share_vars_parallel_executor=share_vars_parallel_executor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （3）Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class QuadrotorAgent(parl.Agent):\n",
    "    def __init__(self, algorithm, obs_dim, act_dim=4):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(QuadrotorAgent, self).__init__(algorithm)\n",
    "\n",
    "        # 注意，在最开始的时候，先完全同步target_model和model的参数\n",
    "        self.alg.sync_target(decay=0)\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.pred_act = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            act = layers.data(\n",
    "                name='act', shape=[self.act_dim], dtype='float32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            _, self.critic_cost = self.alg.learn(obs, act, reward, next_obs,\n",
    "                                                 terminal)\n",
    "\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        # print(act)\n",
    "        # 调整输出到均值附近\n",
    "        act_mean = act.mean(axis=1)\n",
    "        act = act_mean + (act - act_mean) * 0.1\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        feed = {\n",
    "            'obs': obs,\n",
    "            'act': act,\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs,\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        critic_cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.critic_cost])[0]\n",
    "        self.alg.sync_target()\n",
    "\n",
    "        return critic_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step4 Training && Test（训练&&测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_episode(env, agent, rpm):\n",
    "    obs = env.reset()\n",
    "    total_reward, steps = 0, 0\n",
    "    while True:\n",
    "        steps += 1\n",
    "        batch_obs = np.expand_dims(obs, axis=0)\n",
    "        action = agent.predict(batch_obs.astype('float32'))\n",
    "        action = np.squeeze(action)\n",
    "\n",
    "        # 给输出动作增加探索扰动，输出限制在 [-1.0, 1.0] 范围内\n",
    "        action = np.clip(np.random.normal(action, 1.0), -1.0, 1.0)\n",
    "        # 动作映射到对应的 实际动作取值范围 内, action_mapping是从parl.utils那里import进来的函数\n",
    "        action = action_mapping(action, env.action_space.low[0],\n",
    "                                env.action_space.high[0])\n",
    "\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        rpm.append(obs, action, REWARD_SCALE * reward, next_obs, done)\n",
    "\n",
    "        if rpm.size() > MEMORY_WARMUP_SIZE:\n",
    "            batch_obs, batch_action, batch_reward, batch_next_obs, \\\n",
    "                    batch_terminal = rpm.sample_batch(BATCH_SIZE)\n",
    "            critic_cost = agent.learn(batch_obs, batch_action, batch_reward,\n",
    "                                      batch_next_obs, batch_terminal)\n",
    "\n",
    "        obs = next_obs\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, steps\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        total_reward, steps = 0, 0\n",
    "        while True:\n",
    "            batch_obs = np.expand_dims(obs, axis=0)\n",
    "            action = agent.predict(batch_obs.astype('float32'))\n",
    "            action = np.squeeze(action)\n",
    "            action = np.clip(action, -1.0, 1.0)  # the action should be in range [-1.0, 1.0]\n",
    "            action = action_mapping(action, env.action_space.low[0], \n",
    "                                    env.action_space.high[0])\n",
    "\n",
    "            next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "            obs = next_obs\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(total_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 创建环境和Agent，创建经验池，启动训练，定期保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from parl.utils.scheduler import LinearDecayScheduler\n",
    "def main(ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=1, load_model=False, go_steps=1, f_best=''):\n",
    "    # 创建飞行器环境\n",
    "    env = make_env(\"Quadrotor\", task=\"hovering_control\")\n",
    "    env.reset()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "    # 根据parl框架构建agent\n",
    "    ######################################################################\n",
    "    ######################################################################\n",
    "    #\n",
    "    # 6. 请构建agent:  QuadrotorModel, DDPG, QuadrotorAgent三者嵌套\n",
    "    #\n",
    "    ######################################################################\n",
    "    ######################################################################\n",
    "    model = QuadrotorModel(act_dim, model_tag=model_tag)\n",
    "    algorithm = DDPG(model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "    agent = QuadrotorAgent(algorithm, obs_dim, act_dim)\n",
    "\n",
    "    # parl库也为DDPG算法内置了ReplayMemory，可直接从 parl.utils 引入使用\n",
    "    rpm = ReplayMemory(int(MEMORY_SIZE), obs_dim, act_dim)\n",
    "\n",
    "    # 启动训练\n",
    "    logger.info(f'Params: ACTOR_LR={ACTOR_LR}, CRITIC_LR={CRITIC_LR}, model_tag={model_tag}, Pid {os.getpid()}')\n",
    "    test_flag = 0\n",
    "    total_steps = 0\n",
    "    early_stop = 0\n",
    "    last_reward = -1e9\n",
    "    actor_lr_scheduler = LinearDecayScheduler(ACTOR_LR, int(1e7))\n",
    "    critic_lr_scheduler = LinearDecayScheduler(CRITIC_LR, int(1e7))\n",
    "\n",
    "    # load best model\n",
    "    # f1_best = './model_dir/steps_220218_evaluate_reward_3050_ACTOR_LR_0.0005_CRITIC_LR_0.01_model_tag_1.ckpt'\n",
    "    # f2_best = './model_dir/steps_390209_evaluate_reward_8391_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt'\n",
    "    # if model_tag == 1 and load_model == True and os.path.exists(f1_best): \n",
    "    #     agent.restore(f1_best)\n",
    "    #     actor_lr_scheduler.step(step_num=220218)\n",
    "    #     critic_lr_scheduler.step(step_num=220218)\n",
    "    #     logger.info(f'load model success. Pid {os.getpid()}')\n",
    "    # if model_tag == 2 and load_model == True and os.path.exists(f2_best): \n",
    "    #     agent.restore(f2_best)\n",
    "    #     actor_lr_scheduler.step(step_num=390209)\n",
    "    #     critic_lr_scheduler.step(step_num=390209)\n",
    "    #     logger.info(f'load model success. Pid {os.getpid()}')\n",
    "    if load_model == True and os.path.exists(f_best): \n",
    "        agent.restore(f_best)\n",
    "        actor_lr_scheduler.step(step_num=go_steps)\n",
    "        critic_lr_scheduler.step(step_num=go_steps)\n",
    "        logger.info(f'load model success. Pid {os.getpid()}')\n",
    "\n",
    "\n",
    "    while total_steps < TRAIN_TOTAL_STEPS:\n",
    "        train_reward, steps = run_episode(env, agent, rpm)\n",
    "        total_steps += steps\n",
    "        #logger.info('Steps: {} Reward: {} Pid: {}'.format(total_steps, train_reward, os.getpid())) # 打印训练reward\n",
    "\n",
    "        # 可以在这里修改学习率, 可以用 parl.utils.scheduler 中的 LinearDecayScheduler 进行修改，也可以自行修改\n",
    "        agent.alg.actor_lr = max(actor_lr_scheduler.step(step_num=steps), ACTOR_LR/100)\n",
    "        agent.alg.critic_lr = max(critic_lr_scheduler.step(step_num=steps), CRITIC_LR/100)\n",
    "\n",
    "        if total_steps // TEST_EVERY_STEPS >= test_flag: # 每隔一定step数，评估一次模型\n",
    "            while total_steps // TEST_EVERY_STEPS >= test_flag:\n",
    "                test_flag += 1\n",
    "    \n",
    "            evaluate_reward = evaluate(env, agent)\n",
    "            logger.info('Steps {}, Test reward: {}, Pid {}'.format(total_steps, evaluate_reward, os.getpid())) # 打印评估的reward\n",
    "\n",
    "            # 每评估一次，就保存一次模型，以训练的step数命名\n",
    "            if evaluate_reward > 0:\n",
    "                ckpt = f'model_dir/steps_{total_steps}_evaluate_reward_{int(evaluate_reward)}_ACTOR_LR_{ACTOR_LR}_CRITIC_LR_{CRITIC_LR}_model_tag_{model_tag}.ckpt'\n",
    "                agent.save(ckpt)\n",
    "                logger.info(f'Current actor_lr: {agent.alg.actor_lr}  critic_lr: {agent.alg.critic_lr}  Pid {os.getpid()} ckpt {ckpt}')\n",
    "\n",
    "            # early_stop, 超过5%训练进度且连续3次测评reward下降则提前终止\n",
    "            if evaluate_reward > last_reward:\n",
    "                early_stop = 0\n",
    "            else:\n",
    "                early_stop += 1\n",
    "            last_reward = evaluate_reward\n",
    "            # if total_steps > TRAIN_TOTAL_STEPS / 20 and early_stop >= 3:\n",
    "            # if total_steps > 200000 and ((early_stop >= 3 and evaluate_reward < 5000) or evaluate_reward < 0):\n",
    "            if evaluate_reward < 9000 or early_stop >= 2:\n",
    "                logger.info(f'No good results, stop training. Pid {os.getpid()}')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def parallel():\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    print('Parent process %s.' % os.getpid()) \n",
    "\n",
    "    p = Pool(4)\n",
    "    for ACTOR_LR in [0.0001, 0.0002, 0.0005, 0.001, 0.002]:  # 0.0002\n",
    "        for CRITIC_LR in [0.001, 0.005, 0.01]:  # 0.001\n",
    "            for model_tag in [1, 2]:\n",
    "                p.apply_async(main, args=(ACTOR_LR, CRITIC_LR, model_tag))\n",
    "\n",
    "    print('Waiting for all subprocesses done...')\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print('All subprocesses done.')\n",
    "\n",
    "\n",
    "def one(ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=1, load_model=False, go_steps=1, f_best=''):\n",
    "    main(ACTOR_LR, CRITIC_LR, model_tag, load_model, go_steps, f_best)\n",
    "\n",
    "\n",
    "def find_best():\n",
    "    best = -1000000\n",
    "    best_f = ''\n",
    "    for _, _, files in os.walk('./model_dir'):\n",
    "        for f in files:\n",
    "            reward = int(f[f.find('reward')+7:f.find('ACTOR')-1])\n",
    "            if reward >= best:\n",
    "                best = reward\n",
    "                best_f = f\n",
    "    return 'model_dir/' + best_f\n",
    "\n",
    "\n",
    "def fine_tune(ACTOR_LR=0.0002, CRITIC_LR=0.005, episodes=10):\n",
    "    for i in range(episodes):\n",
    "        f_best = find_best()\n",
    "        logger.info(f'Current best: {f_best}, finetune it...') \n",
    "        one(ACTOR_LR=ACTOR_LR, CRITIC_LR=CRITIC_LR, model_tag=2, load_model=True, go_steps=1, f_best=f_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent process 61.\n",
      "\u001b[32m[06-21 15:41:52 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:41:52 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "Waiting for all subprocesses done...\n",
      "\u001b[32m[06-21 15:41:53 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:41:53 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:41:53 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:41:53 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 465, Test reward: 6685.446039921165, Pid 1185\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999068  critic_lr: 0.0049997670000000004  Pid 1185 ckpt model_dir/steps_465_evaluate_reward_6685_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 465, Test reward: 6685.446039921165, Pid 1186\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999068000000002e-05  critic_lr: 0.0004999767  Pid 1186 ckpt model_dir/steps_465_evaluate_reward_6685_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:42:52 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:42:53 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:42:53 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 15:42:53 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:42:53 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:43:53 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 764, Test reward: 9170.04154986786, Pid 1186\n",
      "\u001b[32m[06-21 15:43:53 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.999847e-05  critic_lr: 0.00049996175  Pid 1186 ckpt model_dir/steps_764_evaluate_reward_9170_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:43:54 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 764, Test reward: 9170.04154986786, Pid 1185\n",
      "\u001b[32m[06-21 15:43:54 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.0001999847  critic_lr: 0.0049996175  Pid 1185 ckpt model_dir/steps_764_evaluate_reward_9170_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:46:26 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10340, Test reward: 1379.9027225223633, Pid 1186\n",
      "\u001b[32m[06-21 15:46:26 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9979318e-05  critic_lr: 0.00049948295  Pid 1186 ckpt model_dir/steps_10340_evaluate_reward_1379_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:46:26 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:46:26 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:46:27 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:46:27 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:46:38 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10384, Test reward: -11311.104270565746, Pid 1185\n",
      "\u001b[32m[06-21 15:46:38 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 15:46:38 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:46:39 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:46:39 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 15:47:27 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 651, Test reward: 8656.558998226492, Pid 1186\n",
      "\u001b[32m[06-21 15:47:27 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998696e-05  critic_lr: 0.0004999674000000001  Pid 1186 ckpt model_dir/steps_651_evaluate_reward_8656_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:47:27 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:47:27 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:47:28 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:47:28 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:47:35 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 355, Test reward: 9151.374776678384, Pid 1185\n",
      "\u001b[32m[06-21 15:47:35 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999288  critic_lr: 0.004999822  Pid 1185 ckpt model_dir/steps_355_evaluate_reward_9151_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:48:28 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 644, Test reward: 8529.55728638249, Pid 1186\n",
      "\u001b[32m[06-21 15:48:28 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.999871e-05  critic_lr: 0.00049996775  Pid 1186 ckpt model_dir/steps_644_evaluate_reward_8529_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:48:28 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:48:28 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:48:29 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:48:29 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:49:27 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 251, Test reward: 8505.80715676181, Pid 1186\n",
      "\u001b[32m[06-21 15:49:27 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999496000000003e-05  critic_lr: 0.0004999874  Pid 1186 ckpt model_dir/steps_251_evaluate_reward_8505_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:49:27 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:49:27 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:49:28 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:49:28 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:50:19 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10187, Test reward: 1275.5126159014812, Pid 1185\n",
      "\u001b[32m[06-21 15:50:19 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019979624  critic_lr: 0.004994906  Pid 1185 ckpt model_dir/steps_10187_evaluate_reward_1275_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:50:19 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 15:50:19 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:50:20 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:50:20 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 15:50:29 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 539, Test reward: 8804.052893001215, Pid 1186\n",
      "\u001b[32m[06-21 15:50:29 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.999892e-05  critic_lr: 0.000499973  Pid 1186 ckpt model_dir/steps_539_evaluate_reward_8804_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:50:29 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:50:29 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:50:30 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:50:30 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:51:25 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 969, Test reward: 9298.190795784087, Pid 1185\n",
      "\u001b[32m[06-21 15:51:25 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.0001999806  critic_lr: 0.004999515  Pid 1185 ckpt model_dir/steps_969_evaluate_reward_9298_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:51:28 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 282, Test reward: 8821.709555170886, Pid 1186\n",
      "\u001b[32m[06-21 15:51:28 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999434000000003e-05  critic_lr: 0.0004999858500000001  Pid 1186 ckpt model_dir/steps_282_evaluate_reward_8821_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:51:28 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:51:28 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:51:29 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:51:29 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:52:29 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 612, Test reward: 5664.01580797465, Pid 1186\n",
      "\u001b[32m[06-21 15:52:29 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998774e-05  critic_lr: 0.00049996935  Pid 1186 ckpt model_dir/steps_612_evaluate_reward_5664_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:52:29 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:52:29 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:52:29 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:52:29 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:53:34 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 8198.495523758422, Pid 1186\n",
      "\u001b[32m[06-21 15:53:34 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9997998000000002e-05  critic_lr: 0.00049994995  Pid 1186 ckpt model_dir/steps_1000_evaluate_reward_8198_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:53:34 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:53:34 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:53:35 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:53:35 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:53:49 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10325, Test reward: -3868.8460235712096, Pid 1185\n",
      "\u001b[32m[06-21 15:53:49 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 15:53:49 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:53:50 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:53:50 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 15:54:40 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 9276.994292260704, Pid 1186\n",
      "\u001b[32m[06-21 15:54:40 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9997998000000002e-05  critic_lr: 0.00049994995  Pid 1186 ckpt model_dir/steps_1000_evaluate_reward_9276_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:54:49 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 241, Test reward: 7629.35392748242, Pid 1185\n",
      "\u001b[32m[06-21 15:54:49 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999516  critic_lr: 0.004999879  Pid 1185 ckpt model_dir/steps_241_evaluate_reward_7629_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:54:49 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 15:54:49 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:54:49 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:54:49 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 15:55:48 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 415, Test reward: 8993.557348504975, Pid 1185\n",
      "\u001b[32m[06-21 15:55:48 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999168000000002  critic_lr: 0.004999792  Pid 1185 ckpt model_dir/steps_415_evaluate_reward_8993_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:55:48 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 15:55:48 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:55:49 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:55:49 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 15:56:54 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 9046.107305675545, Pid 1185\n",
      "\u001b[32m[06-21 15:56:54 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019997998  critic_lr: 0.0049994995  Pid 1185 ckpt model_dir/steps_1000_evaluate_reward_9046_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:57:21 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10285, Test reward: -86.59652047256313, Pid 1186\n",
      "\u001b[32m[06-21 15:57:21 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 15:57:21 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:57:22 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 15:57:22 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 15:58:21 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 471, Test reward: 9279.146281952388, Pid 1186\n",
      "\u001b[32m[06-21 15:58:21 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999056000000003e-05  critic_lr: 0.0004999764  Pid 1186 ckpt model_dir/steps_471_evaluate_reward_9279_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 15:59:36 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10600, Test reward: -5745.016533865303, Pid 1185\n",
      "\u001b[32m[06-21 15:59:36 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 15:59:36 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 15:59:37 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 15:59:37 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:00:35 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 562, Test reward: 7849.575752590344, Pid 1185\n",
      "\u001b[32m[06-21 16:00:35 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019998874  critic_lr: 0.0049997185  Pid 1185 ckpt model_dir/steps_562_evaluate_reward_7849_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:00:35 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:00:35 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:00:36 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:00:36 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:01:00 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10131, Test reward: 6469.71978175341, Pid 1186\n",
      "\u001b[32m[06-21 16:01:00 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9979736e-05  critic_lr: 0.0004994934  Pid 1186 ckpt model_dir/steps_10131_evaluate_reward_6469_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:01:00 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:01:00 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:01:01 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:01:01 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:01:36 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 619, Test reward: 9132.185380353536, Pid 1185\n",
      "\u001b[32m[06-21 16:01:36 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.0001999876  critic_lr: 0.00499969  Pid 1185 ckpt model_dir/steps_619_evaluate_reward_9132_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:02:02 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 650, Test reward: 8858.14706954303, Pid 1186\n",
      "\u001b[32m[06-21 16:02:02 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998698e-05  critic_lr: 0.00049996745  Pid 1186 ckpt model_dir/steps_650_evaluate_reward_8858_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:02:02 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:02:02 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:02:02 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:02:02 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:03:03 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 569, Test reward: 8537.327309717388, Pid 1186\n",
      "\u001b[32m[06-21 16:03:03 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998860000000003e-05  critic_lr: 0.0004999715  Pid 1186 ckpt model_dir/steps_569_evaluate_reward_8537_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:03:03 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:03:03 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:03:04 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:03:04 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:03:53 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10410, Test reward: -2029.8669446845379, Pid 1185\n",
      "\u001b[32m[06-21 16:03:53 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:03:53 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:03:54 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:03:54 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:04:07 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 8523.422907637878, Pid 1186\n",
      "\u001b[32m[06-21 16:04:07 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9997998000000002e-05  critic_lr: 0.00049994995  Pid 1186 ckpt model_dir/steps_1000_evaluate_reward_8523_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:04:07 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:04:07 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:04:08 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:04:08 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:05:03 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 8539.065350579805, Pid 1185\n",
      "\u001b[32m[06-21 16:05:03 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019997998  critic_lr: 0.0049994995  Pid 1185 ckpt model_dir/steps_1000_evaluate_reward_8539_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:05:03 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:05:03 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:05:04 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:05:04 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:05:11 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 487, Test reward: 7879.2738170446155, Pid 1186\n",
      "\u001b[32m[06-21 16:05:11 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999024000000002e-05  critic_lr: 0.0004999756  Pid 1186 ckpt model_dir/steps_487_evaluate_reward_7879_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:05:11 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:05:11 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:05:12 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:05:12 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:06:04 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 406, Test reward: 8412.844313763737, Pid 1185\n",
      "\u001b[32m[06-21 16:06:04 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999186  critic_lr: 0.0049997965  Pid 1185 ckpt model_dir/steps_406_evaluate_reward_8412_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:06:04 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:06:04 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_348_evaluate_reward_9451_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:06:05 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:06:05 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:06:12 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 409, Test reward: 9487.398473044292, Pid 1186\n",
      "\u001b[32m[06-21 16:06:12 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999180000000002e-05  critic_lr: 0.0004999795000000001  Pid 1186 ckpt model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:07:09 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 732, Test reward: 6046.172430659799, Pid 1185\n",
      "\u001b[32m[06-21 16:07:09 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019998534  critic_lr: 0.0049996335  Pid 1185 ckpt model_dir/steps_732_evaluate_reward_6046_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:07:09 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:07:09 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:07:10 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:07:10 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:08:10 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 703, Test reward: 9050.53077693441, Pid 1185\n",
      "\u001b[32m[06-21 16:08:10 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019998592  critic_lr: 0.004999648  Pid 1185 ckpt model_dir/steps_703_evaluate_reward_9050_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:09:11 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10343, Test reward: 5920.551911275818, Pid 1186\n",
      "\u001b[32m[06-21 16:09:11 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9979312000000003e-05  critic_lr: 0.0004994828  Pid 1186 ckpt model_dir/steps_10343_evaluate_reward_5920_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:09:11 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:09:11 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:09:12 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:09:12 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:10:11 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 394, Test reward: 9109.972837384475, Pid 1186\n",
      "\u001b[32m[06-21 16:10:11 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.999921e-05  critic_lr: 0.00049998025  Pid 1186 ckpt model_dir/steps_394_evaluate_reward_9109_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:11:03 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10469, Test reward: -8121.57685718932, Pid 1185\n",
      "\u001b[32m[06-21 16:11:03 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:11:03 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:11:04 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:11:04 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:12:11 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 9089.888883145059, Pid 1185\n",
      "\u001b[32m[06-21 16:12:11 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019997998  critic_lr: 0.0049994995  Pid 1185 ckpt model_dir/steps_1000_evaluate_reward_9089_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:13:07 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10399, Test reward: 484.9473085719795, Pid 1186\n",
      "\u001b[32m[06-21 16:13:07 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.99792e-05  critic_lr: 0.00049948  Pid 1186 ckpt model_dir/steps_10399_evaluate_reward_484_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:13:07 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:13:07 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:13:08 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:13:08 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:14:04 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 275, Test reward: 7446.359593011899, Pid 1186\n",
      "\u001b[32m[06-21 16:14:04 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999448e-05  critic_lr: 0.0004999862  Pid 1186 ckpt model_dir/steps_275_evaluate_reward_7446_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:14:04 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:14:04 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:14:05 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:14:05 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:14:50 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10767, Test reward: -4138.782155206421, Pid 1185\n",
      "\u001b[32m[06-21 16:14:50 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:14:50 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:14:51 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:14:51 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:15:00 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 291, Test reward: 8187.875673851781, Pid 1186\n",
      "\u001b[32m[06-21 16:15:00 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999416000000004e-05  critic_lr: 0.0004999854  Pid 1186 ckpt model_dir/steps_291_evaluate_reward_8187_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:15:00 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:15:00 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:15:01 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:15:01 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:15:50 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 439, Test reward: 8677.662520175852, Pid 1185\n",
      "\u001b[32m[06-21 16:15:50 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.0001999912  critic_lr: 0.00499978  Pid 1185 ckpt model_dir/steps_439_evaluate_reward_8677_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:15:50 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:15:50 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:15:51 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:15:51 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:16:02 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 423, Test reward: 9409.49210095464, Pid 1186\n",
      "\u001b[32m[06-21 16:16:02 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999152000000002e-05  critic_lr: 0.0004999788  Pid 1186 ckpt model_dir/steps_423_evaluate_reward_9409_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:16:49 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 463, Test reward: 6966.101853878514, Pid 1185\n",
      "\u001b[32m[06-21 16:16:49 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999072  critic_lr: 0.004999768  Pid 1185 ckpt model_dir/steps_463_evaluate_reward_6966_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:16:49 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:16:49 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:16:50 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:16:50 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:17:49 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 555, Test reward: 8547.396664819093, Pid 1185\n",
      "\u001b[32m[06-21 16:17:49 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019998888  critic_lr: 0.004999722  Pid 1185 ckpt model_dir/steps_555_evaluate_reward_8547_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:17:49 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:17:49 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:17:51 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:17:51 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:18:51 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 244, Test reward: 9397.077909489939, Pid 1185\n",
      "\u001b[32m[06-21 16:18:51 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999510000000002  critic_lr: 0.0049998775  Pid 1185 ckpt model_dir/steps_244_evaluate_reward_9397_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:19:00 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10498, Test reward: 3988.077639135913, Pid 1186\n",
      "\u001b[32m[06-21 16:19:00 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9979002000000002e-05  critic_lr: 0.00049947505  Pid 1186 ckpt model_dir/steps_10498_evaluate_reward_3988_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:19:00 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:19:00 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:19:01 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:19:01 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:20:02 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 653, Test reward: 8783.096583770359, Pid 1186\n",
      "\u001b[32m[06-21 16:20:02 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998692e-05  critic_lr: 0.0004999673  Pid 1186 ckpt model_dir/steps_653_evaluate_reward_8783_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:20:02 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:20:02 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:20:03 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:20:03 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:21:08 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 872, Test reward: 7463.453480869578, Pid 1186\n",
      "\u001b[32m[06-21 16:21:08 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998254000000002e-05  critic_lr: 0.00049995635  Pid 1186 ckpt model_dir/steps_872_evaluate_reward_7463_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:21:08 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:21:08 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:21:10 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:21:10 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:21:56 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10767, Test reward: -8049.167914331588, Pid 1185\n",
      "\u001b[32m[06-21 16:21:56 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:21:56 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:21:57 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:21:58 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:22:10 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 666, Test reward: 9348.307008105365, Pid 1186\n",
      "\u001b[32m[06-21 16:22:10 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998666000000003e-05  critic_lr: 0.00049996665  Pid 1186 ckpt model_dir/steps_666_evaluate_reward_9348_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:23:05 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 8851.262921771626, Pid 1185\n",
      "\u001b[32m[06-21 16:23:05 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019997998  critic_lr: 0.0049994995  Pid 1185 ckpt model_dir/steps_1000_evaluate_reward_8851_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:23:05 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:23:05 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:23:06 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:23:06 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:24:08 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 892, Test reward: 9325.470036877963, Pid 1185\n",
      "\u001b[32m[06-21 16:24:08 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019998214  critic_lr: 0.0049995535  Pid 1185 ckpt model_dir/steps_892_evaluate_reward_9325_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:24:44 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10506, Test reward: 73.0641515451949, Pid 1186\n",
      "\u001b[32m[06-21 16:24:44 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9978986e-05  critic_lr: 0.0004994746500000001  Pid 1186 ckpt model_dir/steps_10506_evaluate_reward_73_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:24:44 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:24:44 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:24:46 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:24:46 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:25:46 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 465, Test reward: 8463.361010698858, Pid 1186\n",
      "\u001b[32m[06-21 16:25:46 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999068000000002e-05  critic_lr: 0.0004999767  Pid 1186 ckpt model_dir/steps_465_evaluate_reward_8463_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:25:46 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:25:46 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:25:47 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:25:47 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:26:42 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10170, Test reward: -2673.649977518356, Pid 1185\n",
      "\u001b[32m[06-21 16:26:42 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:26:42 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:26:43 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:26:43 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:26:44 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 326, Test reward: 9351.588382480784, Pid 1186\n",
      "\u001b[32m[06-21 16:26:44 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999346000000003e-05  critic_lr: 0.00049998365  Pid 1186 ckpt model_dir/steps_326_evaluate_reward_9351_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:27:42 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 504, Test reward: 9182.434737712993, Pid 1185\n",
      "\u001b[32m[06-21 16:27:42 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.0001999899  critic_lr: 0.0049997475  Pid 1185 ckpt model_dir/steps_504_evaluate_reward_9182_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:29:53 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10753, Test reward: 7784.466426345505, Pid 1186\n",
      "\u001b[32m[06-21 16:29:53 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9978492000000003e-05  critic_lr: 0.0004994623  Pid 1186 ckpt model_dir/steps_10753_evaluate_reward_7784_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:29:53 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:29:53 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:29:55 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:29:55 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:30:21 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10559, Test reward: -5225.104037966339, Pid 1185\n",
      "\u001b[32m[06-21 16:30:21 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:30:21 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:30:23 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:30:23 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:30:58 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 940, Test reward: 8920.96925577126, Pid 1186\n",
      "\u001b[32m[06-21 16:30:58 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998118000000003e-05  critic_lr: 0.00049995295  Pid 1186 ckpt model_dir/steps_940_evaluate_reward_8920_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:30:58 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:30:58 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:31:00 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:31:00 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:31:27 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 8133.65805879258, Pid 1185\n",
      "\u001b[32m[06-21 16:31:27 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019997998  critic_lr: 0.0049994995  Pid 1185 ckpt model_dir/steps_1000_evaluate_reward_8133_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:31:27 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:31:27 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:31:28 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:31:28 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:31:56 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 407, Test reward: 9060.798364674283, Pid 1186\n",
      "\u001b[32m[06-21 16:31:56 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999184000000003e-05  critic_lr: 0.0004999796  Pid 1186 ckpt model_dir/steps_407_evaluate_reward_9060_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:32:29 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 592, Test reward: 9320.936226658654, Pid 1185\n",
      "\u001b[32m[06-21 16:32:29 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019998814000000002  critic_lr: 0.0049997035  Pid 1185 ckpt model_dir/steps_592_evaluate_reward_9320_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:34:52 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10459, Test reward: 8677.854822248391, Pid 1186\n",
      "\u001b[32m[06-21 16:34:52 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9979080000000002e-05  critic_lr: 0.000499477  Pid 1186 ckpt model_dir/steps_10459_evaluate_reward_8677_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:34:52 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:34:52 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:34:54 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:34:54 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:35:25 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10972, Test reward: -2183.7900531557025, Pid 1185\n",
      "\u001b[32m[06-21 16:35:25 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:35:25 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:35:27 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:35:27 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:35:54 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 804, Test reward: 9427.759826260928, Pid 1186\n",
      "\u001b[32m[06-21 16:35:54 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.999839e-05  critic_lr: 0.00049995975  Pid 1186 ckpt model_dir/steps_804_evaluate_reward_9427_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:36:27 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 490, Test reward: 8668.527301485501, Pid 1185\n",
      "\u001b[32m[06-21 16:36:27 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999018  critic_lr: 0.0049997545  Pid 1185 ckpt model_dir/steps_490_evaluate_reward_8668_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:36:27 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:36:27 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:36:29 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:36:29 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:37:30 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 1000, Test reward: 6172.352572570883, Pid 1185\n",
      "\u001b[32m[06-21 16:37:30 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019997998  critic_lr: 0.0049994995  Pid 1185 ckpt model_dir/steps_1000_evaluate_reward_6172_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:37:30 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:37:30 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:37:31 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:37:31 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:38:28 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 515, Test reward: 8239.004919173636, Pid 1185\n",
      "\u001b[32m[06-21 16:38:28 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019998968  critic_lr: 0.004999742  Pid 1185 ckpt model_dir/steps_515_evaluate_reward_8239_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:38:28 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:38:28 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:38:30 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:38:30 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:38:32 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10176, Test reward: 7075.197256917479, Pid 1186\n",
      "\u001b[32m[06-21 16:38:32 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9979646e-05  critic_lr: 0.00049949115  Pid 1186 ckpt model_dir/steps_10176_evaluate_reward_7075_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:38:32 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:38:32 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:38:33 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:38:33 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:39:24 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 225, Test reward: 8035.588781935861, Pid 1185\n",
      "\u001b[32m[06-21 16:39:24 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999548000000002  critic_lr: 0.004999887000000001  Pid 1185 ckpt model_dir/steps_225_evaluate_reward_8035_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:39:24 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:39:24 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:39:25 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:39:25 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:39:35 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 498, Test reward: 7793.111858798266, Pid 1186\n",
      "\u001b[32m[06-21 16:39:35 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9999002e-05  critic_lr: 0.00049997505  Pid 1186 ckpt model_dir/steps_498_evaluate_reward_7793_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:39:35 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:39:35 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:39:37 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=2e-05, CRITIC_LR=0.0005, model_tag=2, Pid 1186\n",
      "\u001b[32m[06-21 16:39:37 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1186\n",
      "\u001b[32m[06-21 16:40:26 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 404, Test reward: 7454.41095433276, Pid 1185\n",
      "\u001b[32m[06-21 16:40:26 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.0001999919  critic_lr: 0.0049997975  Pid 1185 ckpt model_dir/steps_404_evaluate_reward_7454_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:40:26 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:40:26 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:40:27 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:40:27 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:40:36 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 737, Test reward: 8749.9581902239, Pid 1186\n",
      "\u001b[32m[06-21 16:40:36 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 1.9998524e-05  critic_lr: 0.0004999631  Pid 1186 ckpt model_dir/steps_737_evaluate_reward_8749_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:40:36 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1186\n",
      "\u001b[32m[06-21 16:41:23 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 354, Test reward: 8366.363105843182, Pid 1185\n",
      "\u001b[32m[06-21 16:41:23 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.00019999290000000003  critic_lr: 0.0049998225  Pid 1185 ckpt model_dir/steps_354_evaluate_reward_8366_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:41:23 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "\u001b[32m[06-21 16:41:23 MainThread @<ipython-input-74-b64c3768500b>:37]\u001b[0m Current best: model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt, finetune it...\n",
      "\u001b[32m[06-21 16:41:24 MainThread @<ipython-input-69-6450d574507d>:26]\u001b[0m Params: ACTOR_LR=0.0002, CRITIC_LR=0.005, model_tag=2, Pid 1185\n",
      "\u001b[32m[06-21 16:41:24 MainThread @<ipython-input-69-6450d574507d>:51]\u001b[0m load model success. Pid 1185\n",
      "\u001b[32m[06-21 16:42:17 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 334, Test reward: 9281.490317152357, Pid 1185\n",
      "\u001b[32m[06-21 16:42:17 MainThread @<ipython-input-69-6450d574507d>:74]\u001b[0m Current actor_lr: 0.0001999933  critic_lr: 0.0049998325  Pid 1185 ckpt model_dir/steps_334_evaluate_reward_9281_ACTOR_LR_0.0002_CRITIC_LR_0.005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:44:55 MainThread @<ipython-input-69-6450d574507d>:68]\u001b[0m Steps 10471, Test reward: -4139.298229069696, Pid 1185\n",
      "\u001b[32m[06-21 16:44:55 MainThread @<ipython-input-69-6450d574507d>:85]\u001b[0m No good results, stop training. Pid 1185\n",
      "All subprocesses done.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "print('Parent process %s.' % os.getpid()) \n",
    "\n",
    "p = Pool(2) \n",
    "p.apply_async(fine_tune, args=(0.0002, 0.005, 30))\n",
    "p.apply_async(fine_tune, args=(0.00002, 0.0005, 30))\n",
    "\n",
    "print('Waiting for all subprocesses done...')\n",
    "p.close()\n",
    "p.join()\n",
    "print('All subprocesses done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验收测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 7. 请选择你训练的最好的一次模型文件做评估\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "\n",
    "def test(total_steps, evaluate_reward, ACTOR_LR=0.0002, CRITIC_LR=0.001, model_tag=1, render=False):\n",
    "    # 请设置ckpt为你训练中效果最好的一次评估保存的模型文件名称\n",
    "    ckpt = f'model_dir/steps_{total_steps}_evaluate_reward_{int(evaluate_reward)}_ACTOR_LR_{ACTOR_LR}_CRITIC_LR_{CRITIC_LR}_model_tag_{model_tag}.ckpt'\n",
    "    env = make_env(\"Quadrotor\", task=\"hovering_control\")\n",
    "    env.reset()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    model = QuadrotorModel(act_dim, model_tag=model_tag)\n",
    "    algorithm = DDPG(model, gamma=GAMMA, tau=TAU, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR)\n",
    "    agent = QuadrotorAgent(algorithm, obs_dim, act_dim)\n",
    "    # 加载模型\n",
    "    if os.path.exists(ckpt):\n",
    "        agent.restore(ckpt)\n",
    "        logger.info(f'Test Model file {ckpt}')\n",
    "    else:\n",
    "        logger.info(f'No Model file {ckpt}')\n",
    "        return -1\n",
    "    evaluate_reward = evaluate(env, agent, render=render)\n",
    "    logger.info('Evaluate reward: {}'.format(evaluate_reward)) # 打印评估的reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06-21 16:48:32 MainThread @<ipython-input-77-466d854624d9>:22]\u001b[0m Test Model file model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
      "\u001b[32m[06-21 16:49:23 MainThread @<ipython-input-77-466d854624d9>:27]\u001b[0m Evaluate reward: 9348.914547714336\n"
     ]
    }
   ],
   "source": [
    "def test_best():\n",
    "    best = -1000000\n",
    "    best_f = ''\n",
    "    for _, _, files in os.walk('./model_dir'):\n",
    "        for f in files:\n",
    "            reward = int(f[f.find('reward')+7:f.find('ACTOR')-1])\n",
    "            if reward >= best:\n",
    "                best = reward\n",
    "                best_f = f \n",
    "    res = best_f.split('_') \n",
    "    test(res[1], res[4], float(res[7]), float(res[10]), int(res[-1].split('.')[0]), render=False)\n",
    "\n",
    "test_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "#### 1. 对训练代码进行了一定优化\n",
    "- 通过设置early_stop判断条件，对大量的参数组合进行快速验证\n",
    "- 基于较优组合进行后续微调\n",
    "- 修改predict函数，使得输出值接近\n",
    "```\n",
    "    def predict(self, obs):\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        act = self.fluid_executor.run(\n",
    "            self.pred_program, feed={'obs': obs},\n",
    "            fetch_list=[self.pred_act])[0]\n",
    "        # print(act)\n",
    "        # 调整输出到均值附近\n",
    "        act_mean = act.mean(axis=1)\n",
    "        act = act_mean + (act - act_mean) * 0.1\n",
    "        return act\n",
    "```      \n",
    "- 增加自适应学习率\n",
    "```\n",
    "        # 可以用 parl.utils.scheduler 中的 LinearDecayScheduler 修改学习率\n",
    "        agent.alg.actor_lr = max(actor_lr_scheduler.step(step_num=steps), ACTOR_LR/100)\n",
    "        agent.alg.critic_lr = max(critic_lr_scheduler.step(step_num=steps), CRITIC_LR/100)\n",
    "```\n",
    "\n",
    "#### 2. 对大量参数组合进行了实验\n",
    "- 模型网络结构方面，实验了隐层为1层和2层fc两种，1层效果较差，最终选择了2层结构\n",
    "- 按正态分布进行权重初始化，不过很难看出是否有明显效果\n",
    "- 对初始学习率，最开始尝试时按以下组合进行了实验，最优组合为ACTOR_LR=0.0002，CRITIC_LR=0.005\n",
    "```\n",
    "    for ACTOR_LR in [0.0001, 0.0002, 0.0005, 0.001, 0.002]:  # 0.0002\n",
    "        for CRITIC_LR in [0.001, 0.005, 0.01]:  # 0.001\n",
    "```\n",
    "之后微调，按(0.0002, 0.005)和(0.00002, 0.0005)进行的尝试，感觉效果差不多，目前最优的是用(0.00002, 0.0005)微调的。\n",
    "\n",
    "#### 3. 训练过程\n",
    "- Baidu AIStudio\n",
    "- Google Colab\n",
    "- MacBook Pro\n",
    "> 用各种机器进行了训练，对于不好的参数组合，全部100万步训练完成甚至都见不到reward为正的情况，对于目前定下的组合，大概训练了20万步后reward达到5000。\n",
    "> 在39万步时，下载了中间结果，后来Colab超时断开连接了，所以后续是基于这个39万步的结果继续微调。微调波动也很大，前几万步结果还好，后续会突然变负，变负就终止。\n",
    "\n",
    "#### 4. 最终结果\n",
    "- 最优模型为 model_dir/steps_409_evaluate_reward_9487_ACTOR_LR_2e-05_CRITIC_LR_0.0005_model_tag_2.ckpt\n",
    "- **Evaluate reward: 9348.914547714336**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
